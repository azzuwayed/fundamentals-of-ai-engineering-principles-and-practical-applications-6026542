{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e10637d",
   "metadata": {},
   "source": [
    "# Deconstructing Local LLMs\n",
    "\n",
    "When we download an LLM for local use, it comes with several essential components that work together to make text generation possible. In this lesson, we'll explore these components, understand what they do, and learn how they fit together.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Identify the key files in a local LLM directory\n",
    "- Understand the purpose of model configuration files\n",
    "- Examine model weights and architecture\n",
    "- Explore tokenizer components and how tokenization works\n",
    "- Understand how text is converted to tokens and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7ac0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model already exists in ./downloaded_model\n",
      "  Loading from local directory...\n",
      "✓ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "## Setup: Download and Save a Model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "\n",
    "# Set the directory where we'll save the model\n",
    "save_directory = \"./downloaded_model\"  \n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# Check if model already exists locally\n",
    "if os.path.exists(save_directory) and os.listdir(save_directory):\n",
    "    print(f\"✓ Model already exists in {save_directory}\")\n",
    "    print(\"  Loading from local directory...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "else:\n",
    "    # Create directory and download model\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading {model_name} from Hugging Face Hub...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Save the model to our local directory\n",
    "    print(f\"Saving model to {save_directory}...\")\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(\"✓ Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d244ca",
   "metadata": {},
   "source": [
    "## Exploring the Model Files\n",
    "\n",
    "Let's see what files were created when we downloaded the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e21c314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the model directory:\n",
      "\n",
      "  config.json                          0.00 MB\n",
      "  generation_config.json               0.00 MB\n",
      "  merges.txt                           0.44 MB\n",
      "  model.safetensors                  312.48 MB\n",
      "  special_tokens_map.json              0.00 MB\n",
      "  tokenizer.json                       3.39 MB\n",
      "  tokenizer_config.json                0.00 MB\n",
      "  vocab.json                           0.76 MB\n"
     ]
    }
   ],
   "source": [
    "# List all files in the model directory\n",
    "files = os.listdir(save_directory)\n",
    "print(\"Files in the model directory:\\n\")\n",
    "for file in sorted(files):\n",
    "    # Get file size in MB\n",
    "    file_path = os.path.join(save_directory, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"  {file:<30} {file_size:>10.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e1914",
   "metadata": {},
   "source": [
    "## Understanding the Key Components\n",
    "\n",
    "The files we see in the model directory can be grouped into three main categories:\n",
    "\n",
    "### 1. Model Configuration\n",
    "- `config.json` - Model architecture and hyperparameters\n",
    "- `generation_config.json` - Default generation parameters\n",
    "\n",
    "### 2. Model Weights\n",
    "- `model.safetensors` or `pytorch_model.bin` - The actual trained parameters\n",
    "\n",
    "### 3. Tokenizer Components\n",
    "- `tokenizer_config.json` - Tokenizer settings\n",
    "- `vocab.json` - Vocabulary mapping tokens to IDs\n",
    "- `merges.txt` - Byte-Pair Encoding (BPE) merges for subword tokenization\n",
    "- `tokenizer.json` - Optimized tokenizer data\n",
    "- `special_tokens_map.json` - Defines special tokens like `<|endoftext|>`\n",
    "\n",
    "Let's examine each of these components in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52d08f",
   "metadata": {},
   "source": [
    "## 1. Model Configuration (config.json)\n",
    "\n",
    "The `config.json` file contains essential information about the model architecture and hyperparameters. This tells the framework how to construct the model's neural network layers.\n",
    "\n",
    "**Key parameters:**\n",
    "- `model_type` - The architecture family (e.g., GPT-2, BERT)\n",
    "- `vocab_size` - Number of tokens in the vocabulary\n",
    "- `n_positions` - Maximum sequence length the model can handle\n",
    "- `n_embd` - Dimension of embeddings and hidden layers\n",
    "- `n_layer` - Number of transformer layers/blocks\n",
    "- `n_head` - Number of attention heads in each layer\n",
    "- `activation_function` - Non-linearity used (e.g., GELU, ReLU)\n",
    "- `*_pdrop` - Dropout probabilities for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b890c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key model configuration parameters:\n",
      "\n",
      "  model_type                gpt2\n",
      "  vocab_size                50257\n",
      "  n_positions               1024\n",
      "  n_embd                    768\n",
      "  n_layer                   6\n",
      "  n_head                    12\n",
      "  activation_function       gelu_new\n",
      "  resid_pdrop               0.1\n",
      "  embd_pdrop                0.1\n",
      "  attn_pdrop                0.1\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the config.json file\n",
    "config_path = os.path.join(save_directory, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Display key configuration parameters\n",
    "print(\"Key model configuration parameters:\\n\")\n",
    "important_params = [\n",
    "    \"model_type\", \"vocab_size\", \"n_positions\", \"n_embd\", \"n_layer\", \"n_head\", \n",
    "    \"activation_function\", \"resid_pdrop\", \"embd_pdrop\", \"attn_pdrop\"\n",
    "]\n",
    "for param in important_params:\n",
    "    if param in config:\n",
    "        print(f\"  {param:<25} {config[param]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258d386",
   "metadata": {},
   "source": [
    "## 2. Model Weights\n",
    "\n",
    "The model weights are stored in one of these formats:\n",
    "- `pytorch_model.bin` - PyTorch's native format\n",
    "- `model.safetensors` - A newer, safer format for storing tensors (preferred)\n",
    "\n",
    "These files contain the actual trained parameters of the model - the weights and biases learned during training. Let's examine what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6eb1d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found weights file: model.safetensors\n",
      "\n",
      "Model weight matrices (first 10):\n",
      "\n",
      "Layer Name                                         Shape                Sample Values\n",
      "====================================================================================================\n",
      "transformer.h.0.attn.c_attn.bias                   torch.Size([2304])   [0.4693, -0.4959, -0.4158, ...]\n",
      "transformer.h.0.attn.c_attn.weight                 torch.Size([768, 2304]) [-0.4988, -0.1990, -0.1046, ...]\n",
      "transformer.h.0.attn.c_proj.bias                   torch.Size([768])    [0.1617, -0.1644, -0.1561, ...]\n",
      "transformer.h.0.attn.c_proj.weight                 torch.Size([768, 768]) [0.2581, -0.1660, 0.0625, ...]\n",
      "transformer.h.0.ln_1.bias                          torch.Size([768])    [0.0048, 0.0129, -0.0190, ...]\n",
      "transformer.h.0.ln_1.weight                        torch.Size([768])    [0.2195, 0.1853, 0.1572, ...]\n",
      "transformer.h.0.ln_2.bias                          torch.Size([768])    [0.0385, 0.0581, 0.0133, ...]\n",
      "transformer.h.0.ln_2.weight                        torch.Size([768])    [0.1342, 0.2176, 0.2098, ...]\n",
      "transformer.h.0.mlp.c_fc.bias                      torch.Size([3072])   [0.0458, -0.0849, -0.1333, ...]\n",
      "transformer.h.0.mlp.c_fc.weight                    torch.Size([768, 3072]) [0.1239, 0.1034, -0.0096, ...]\n"
     ]
    }
   ],
   "source": [
    "# Find the weights file\n",
    "weights_file = None\n",
    "for file in files:\n",
    "    if file.endswith(\".bin\") or file.endswith(\".safetensors\"):\n",
    "        weights_file = file\n",
    "        break\n",
    "\n",
    "if weights_file:\n",
    "    print(f\"Found weights file: {weights_file}\\n\")\n",
    "\n",
    "    if weights_file.endswith(\".bin\"):\n",
    "        weights_path = os.path.join(save_directory, weights_file)\n",
    "        state_dict = torch.load(weights_path)\n",
    "\n",
    "        print(\"Model weight matrices (first 10):\\n\")\n",
    "        print(f\"{'Layer Name':<50} {'Shape':<20} {'Sample Values'}\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        for name, tensor in list(state_dict.items())[:10]:\n",
    "            preview = tensor.flatten()[:3].tolist()\n",
    "            preview_str = f\"[{preview[0]:.4f}, {preview[1]:.4f}, {preview[2]:.4f}, ...]\"\n",
    "            print(f\"{name:<50} {str(tensor.shape):<20} {preview_str}\")\n",
    "\n",
    "    elif weights_file.endswith(\".safetensors\"):\n",
    "        weights_path = os.path.join(save_directory, weights_file)\n",
    "        with safe_open(weights_path, framework=\"pt\") as f:\n",
    "            tensor_names = list(f.keys())[:10]\n",
    "\n",
    "            print(\"Model weight matrices (first 10):\\n\")\n",
    "            print(f\"{'Layer Name':<50} {'Shape':<20} {'Sample Values'}\")\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "            for name in tensor_names:\n",
    "                tensor = f.get_tensor(name)\n",
    "                preview = tensor.flatten()[:3].tolist()\n",
    "                preview_str = f\"[{preview[0]:.4f}, {preview[1]:.4f}, {preview[2]:.4f}, ...]\"\n",
    "                print(f\"{name:<50} {str(tensor.shape):<20} {preview_str}\")\n",
    "else:\n",
    "    print(\"No weights file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9c87d",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Components\n",
    "\n",
    "The tokenizer is responsible for converting text into token IDs that the model can process, and vice versa.\n",
    "\n",
    "### Tokenizer Configuration (tokenizer_config.json)\n",
    "\n",
    "**Key settings:**\n",
    "- `model_max_length` - Maximum sequence length the tokenizer will handle\n",
    "- `bos_token`, `eos_token`, `unk_token` - Special tokens for beginning/end of sequence and unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8f11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Configuration:\n",
      "\n",
      "  add_prefix_space: False\n",
      "  added_tokens_decoder: dict with 1 entries\n",
      "  bos_token: <|endoftext|>\n",
      "  clean_up_tokenization_spaces: False\n",
      "  eos_token: <|endoftext|>\n",
      "  extra_special_tokens: {}\n",
      "  model_max_length: 1024\n",
      "  tokenizer_class: GPT2Tokenizer\n",
      "  unk_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Examine tokenizer_config.json\n",
    "tokenizer_config_path = os.path.join(save_directory, \"tokenizer_config.json\")\n",
    "if os.path.exists(tokenizer_config_path):\n",
    "    with open(tokenizer_config_path, \"r\") as f:\n",
    "        tokenizer_config = json.load(f)\n",
    "\n",
    "    print(\"Tokenizer Configuration:\\n\")\n",
    "    for key, value in tokenizer_config.items():\n",
    "        # Format long values more nicely\n",
    "        if isinstance(value, dict) and len(str(value)) > 80:\n",
    "            print(f\"  {key}: {type(value).__name__} with {len(value)} entries\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No tokenizer_config.json found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b0d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50,257 tokens\n",
      "\n",
      "Sample tokens (first 20):\n",
      "      0: '!'\n",
      "      1: '\"'\n",
      "      2: '#'\n",
      "      3: '$'\n",
      "      4: '%'\n",
      "      5: '&'\n",
      "      6: \"'\"\n",
      "      7: '('\n",
      "      8: ')'\n",
      "      9: '*'\n",
      "     10: '+'\n",
      "     11: ','\n",
      "     12: '-'\n",
      "     13: '.'\n",
      "     14: '/'\n",
      "     15: '0'\n",
      "     16: '1'\n",
      "     17: '2'\n",
      "     18: '3'\n",
      "     19: '4'\n",
      "\n",
      "Some interesting word tokens:\n",
      "  31373: 'hello'\n",
      "   6894: 'world'\n",
      "  20185: 'AI'\n",
      "  19849: 'model'\n",
      "\n",
      "Special tokens:\n",
      "  50256: '<|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "### Vocabulary (vocab.json)\n",
    "\n",
    "# Examine vocab.json\n",
    "vocab_path = os.path.join(save_directory, \"vocab.json\")\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocab):,} tokens\\n\")\n",
    "\n",
    "    # Show the first 20 tokens\n",
    "    print(\"Sample tokens (first 20):\")\n",
    "    for i, (token, token_id) in enumerate(list(vocab.items())[:20]):\n",
    "        print(f\"  {token_id:5d}: {repr(token)}\")\n",
    "\n",
    "    # Show some interesting tokens\n",
    "    print(\"\\nSome interesting word tokens:\")\n",
    "    interesting_tokens = [\"hello\", \"world\", \"programming\", \"AI\", \"model\"]\n",
    "    for token in interesting_tokens:\n",
    "        if token in vocab:\n",
    "            print(f\"  {vocab[token]:5d}: {repr(token)}\")\n",
    "\n",
    "    # Show special tokens\n",
    "    print(\"\\nSpecial tokens:\")\n",
    "    special_tokens = [\"<|endoftext|>\", \"<|pad|>\", \"<|mask|>\"]\n",
    "    for token in special_tokens:\n",
    "        if token in vocab:\n",
    "            print(f\"  {vocab[token]:5d}: {repr(token)}\")\n",
    "else:\n",
    "    print(\"No vocab.json found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb48526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BPE merges: 50,001\n",
      "\n",
      "First 10 BPE merges:\n",
      "  #version: 0.2\n",
      "  Ġ t\n",
      "  Ġ a\n",
      "  h e\n",
      "  i n\n",
      "  r e\n",
      "  o n\n",
      "  Ġt he\n",
      "  e r\n",
      "  Ġ s\n",
      "\n",
      "================================================================================\n",
      "Understanding BPE (Byte-Pair Encoding) merges:\n",
      "================================================================================\n",
      "• Each line shows two tokens that can be merged into one\n",
      "• Merges are applied in order during tokenization\n",
      "• This allows the model to handle unknown words by breaking them into subwords\n",
      "• The 'Ġ' symbol represents a space character\n"
     ]
    }
   ],
   "source": [
    "### BPE Merges (merges.txt)\n",
    "\n",
    "# Examine merges.txt (BPE merges)\n",
    "merges_path = os.path.join(save_directory, \"merges.txt\")\n",
    "if os.path.exists(merges_path):\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        merges = f.readlines()\n",
    "\n",
    "    print(f\"Number of BPE merges: {len(merges):,}\\n\")\n",
    "\n",
    "    # Show the first few merges\n",
    "    print(\"First 10 BPE merges:\")\n",
    "    for i, merge in enumerate(merges[:10]):\n",
    "        print(f\"  {merge.strip()}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Understanding BPE (Byte-Pair Encoding) merges:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"• Each line shows two tokens that can be merged into one\")\n",
    "    print(\"• Merges are applied in order during tokenization\")\n",
    "    print(\"• This allows the model to handle unknown words by breaking them into subwords\")\n",
    "    print(\"• The 'Ġ' symbol represents a space character\")\n",
    "else:\n",
    "    print(\"No merges.txt found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4d148",
   "metadata": {},
   "source": [
    "## Tokenization in Action\n",
    "\n",
    "Let's see how the tokenizer works with a real example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f03f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "  The quick brown fox jumps over the lazy dog. This is an example of tokenization in NLP.\n",
      "\n",
      "Tokenized into 21 tokens:\n",
      "  ['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.', 'ĠThis', 'Ġis', 'Ġan', 'Ġexample', 'Ġof', 'Ġtoken', 'ization', 'Ġin', 'ĠN', 'LP', '.']\n",
      "\n",
      "Converted to 21 token IDs:\n",
      "  [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13, 770, 318, 281, 1672, 286, 11241, 1634, 287, 399, 19930, 13]\n",
      "\n",
      "Token → ID mapping:\n",
      "----------------------------------------\n",
      "  'The'                → 464\n",
      "  'Ġquick'             → 2068\n",
      "  'Ġbrown'             → 7586\n",
      "  'Ġfox'               → 21831\n",
      "  'Ġjumps'             → 18045\n",
      "  'Ġover'              → 625\n",
      "  'Ġthe'               → 262\n",
      "  'Ġlazy'              → 16931\n",
      "  'Ġdog'               → 3290\n",
      "  '.'                  → 13\n",
      "  'ĠThis'              → 770\n",
      "  'Ġis'                → 318\n",
      "  'Ġan'                → 281\n",
      "  'Ġexample'           → 1672\n",
      "  'Ġof'                → 286\n",
      "  'Ġtoken'             → 11241\n",
      "  'ization'            → 1634\n",
      "  'Ġin'                → 287\n",
      "  'ĠN'                 → 399\n",
      "  'LP'                 → 19930\n",
      "  '.'                  → 13\n",
      "\n",
      "Decoded text:\n",
      "  The quick brown fox jumps over the lazy dog. This is an example of tokenization in NLP.\n",
      "\n",
      "✓ Round-trip successful! (Original → Tokens → IDs → Text)\n"
     ]
    }
   ],
   "source": [
    "# Reload the tokenizer to ensure we're using the local files\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Define a sample text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. This is an example of tokenization in NLP.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = local_tokenizer.tokenize(sample_text)\n",
    "token_ids = local_tokenizer.encode(sample_text)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Original text:\\n  {sample_text}\\n\")\n",
    "print(f\"Tokenized into {len(tokens)} tokens:\")\n",
    "print(f\"  {tokens}\\n\")\n",
    "\n",
    "print(f\"Converted to {len(token_ids)} token IDs:\")\n",
    "print(f\"  {token_ids}\\n\")\n",
    "\n",
    "# Show token to ID mapping\n",
    "print(\"Token → ID mapping:\")\n",
    "print(\"-\" * 40)\n",
    "for token, token_id in zip(tokens, token_ids):\n",
    "    print(f\"  {repr(token):20} → {token_id}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = local_tokenizer.decode(token_ids)\n",
    "print(f\"\\nDecoded text:\\n  {decoded_text}\")\n",
    "print(\"\\n✓ Round-trip successful! (Original → Tokens → IDs → Text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752ac25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "When you download a local LLM, you get a complete package with three essential components:\n",
    "\n",
    "### 1. Model Architecture and Configuration\n",
    "- **config.json** - Defines the neural network architecture (layers, attention heads, dimensions)\n",
    "- **generation_config.json** - Default parameters for text generation (temperature, top_p, max length)\n",
    "\n",
    "### 2. Model Weights\n",
    "- **model.safetensors** or **pytorch_model.bin** - All trained neural network weights (the actual learned parameters)\n",
    "\n",
    "### 3. Tokenizer Components\n",
    "- **vocab.json** - Maps text tokens to their corresponding IDs\n",
    "- **merges.txt** - BPE merge rules that determine how characters combine into subword tokens\n",
    "- **tokenizer.json** - Optimized version combining vocabulary and merge rules\n",
    "- **tokenizer_config.json** - Settings for tokenizer behavior\n",
    "- **special_tokens_map.json** - Defines special tokens like `<|endoftext|>`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Configuration files** tell you how the model is structured\n",
    "2. **Weight files** contain the learned knowledge (and are the largest files)\n",
    "3. **Tokenizer files** enable text ↔ token ID conversion\n",
    "4. **All components must work together** for the model to function properly\n",
    "5. **BPE tokenization** enables handling of unknown words through subword units"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentals-of-ai-engineering-principles-and-practical-applications-6026542",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
