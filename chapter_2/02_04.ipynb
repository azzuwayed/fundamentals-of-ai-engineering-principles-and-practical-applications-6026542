{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Putting the LLM Pipeline Together: Step by Step\n",
    "\n",
    "In this notebook, we'll walk through the complete process of text generation with a local LLM, examining each step in detail.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the complete text generation pipeline from input to output\n",
    "- Trace how text flows through tokenization, model processing, and decoding\n",
    "- Examine model predictions and probability distributions\n",
    "- Compare different token selection strategies (greedy vs. sampling)\n",
    "- Adjust generation parameters to control output quality\n",
    "\n",
    "## The Pipeline Overview\n",
    "\n",
    "**Input text → Tokenization → Token IDs → Model processing → Logits → Token selection → Output text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ca502",
   "metadata": {},
   "source": [
    "## Setup: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model already exists in ./downloaded_model\n",
      "  Loading from local directory...\n",
      "✓ Model and tokenizer ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set the directory where we'll save the model\n",
    "save_directory = \"./downloaded_model\"\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# Check if model already exists locally\n",
    "if os.path.exists(save_directory) and os.listdir(save_directory):\n",
    "    print(f\"✓ Model already exists in {save_directory}\")\n",
    "    print(\"  Loading from local directory...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "else:\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    print(f\"Downloading {model_name} from Hugging Face Hub...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    print(f\"Saving model to {save_directory}...\")\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# Configure tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Model and tokenizer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-md",
   "metadata": {},
   "source": [
    "## Step 1: Input Text\n",
    "\n",
    "Let's start with a simple prompt that we'll use throughout this demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "input-text",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: 'Artificial intelligence (الذكاء الاصطناعي) is transforming'\n"
     ]
    }
   ],
   "source": [
    "# Our starting prompt\n",
    "prompt = \"Artificial intelligence (الذكاء الاصطناعي) is transforming\"\n",
    "print(f\"Input prompt: '{prompt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-md",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization - Breaking Text into Pieces\n",
    "\n",
    "The tokenizer breaks our text into smaller units (tokens) that the model can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tokenization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result (23 tokens):\n",
      "\n",
      "  Token 1: 'Art'\n",
      "  Token 2: 'ificial'\n",
      "  Token 3: 'Ġintelligence'\n",
      "  Token 4: 'Ġ('\n",
      "  Token 5: 'Ø§ÙĦ'\n",
      "  Token 6: 'Ø'\n",
      "  Token 7: '°'\n",
      "  Token 8: 'Ù'\n",
      "  Token 9: 'ĥ'\n",
      "  Token 10: 'Ø§Ø'\n",
      "  Token 11: '¡'\n",
      "  Token 12: 'ĠØ§ÙĦ'\n",
      "  Token 13: 'Ø§Ø'\n",
      "  Token 14: 'µ'\n",
      "  Token 15: 'Ø'\n",
      "  Token 16: '·'\n",
      "  Token 17: 'ÙĨ'\n",
      "  Token 18: 'Ø§Ø'\n",
      "  Token 19: '¹'\n",
      "  Token 20: 'ÙĬ'\n",
      "  Token 21: ')'\n",
      "  Token 22: 'Ġis'\n",
      "  Token 23: 'Ġtransforming'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "\n",
    "print(f\"Tokenization result ({len(tokens)} tokens):\\n\")\n",
    "for i, token in enumerate(tokens, 1):\n",
    "    print(f\"  Token {i}: {repr(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization-explanation",
   "metadata": {},
   "source": [
    "### Understanding Tokenization\n",
    "\n",
    "The tokenizer has split our input text into tokens. Key observations:\n",
    "\n",
    "- **'Ġ' prefix** - Represents a space before the word (GPT-2's way of encoding spaces)\n",
    "- **Subword splitting** - Common words like \"transforming\" stay as single tokens\n",
    "- **Vocabulary efficiency** - Rare words would be split into multiple subword tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-md",
   "metadata": {},
   "source": [
    "## Step 3: Converting Tokens to IDs\n",
    "\n",
    "Next, each token is converted to its corresponding numeric ID from the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "token-to-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token → ID conversion:\n",
      "\n",
      "  'Art'                → 8001\n",
      "  'ificial'            → 9542\n",
      "  'Ġintelligence'      → 4430\n",
      "  'Ġ('                 → 357\n",
      "  'Ø§ÙĦ'               → 23525\n",
      "  'Ø'                  → 148\n",
      "  '°'                  → 108\n",
      "  'Ù'                  → 149\n",
      "  'ĥ'                  → 225\n",
      "  'Ø§Ø'                → 34247\n",
      "  '¡'                  → 94\n",
      "  'ĠØ§ÙĦ'              → 28981\n",
      "  'Ø§Ø'                → 34247\n",
      "  'µ'                  → 113\n",
      "  'Ø'                  → 148\n",
      "  '·'                  → 115\n",
      "  'ÙĨ'                 → 23338\n",
      "  'Ø§Ø'                → 34247\n",
      "  '¹'                  → 117\n",
      "  'ÙĬ'                 → 22654\n",
      "  ')'                  → 8\n",
      "  'Ġis'                → 318\n",
      "  'Ġtransforming'      → 25449\n",
      "\n",
      "Model input tensor shape: torch.Size([1, 23])\n",
      "Model input tensor:\n",
      "  tensor([[ 8001,  9542,  4430,   357, 23525,   148,   108,   149,   225, 34247,\n",
      "            94, 28981, 34247,   113,   148,   115, 23338, 34247,   117, 22654,\n",
      "             8,   318, 25449]])\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to IDs\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")[0].tolist()\n",
    "\n",
    "print(\"Token → ID conversion:\\n\")\n",
    "for token, token_id in zip(tokens, input_ids):\n",
    "    print(f\"  {repr(token):20} → {token_id}\")\n",
    "\n",
    "# Show the tensor format that will be input to the model\n",
    "model_input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"\\nModel input tensor shape: {model_input_ids.shape}\")\n",
    "print(f\"Model input tensor:\\n  {model_input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ids-explanation",
   "metadata": {},
   "source": [
    "### Understanding Token IDs\n",
    "\n",
    "Each token has been converted to a numeric ID from the model's vocabulary:\n",
    "- The model doesn't process text directly - only these numeric IDs\n",
    "- IDs are formatted as a PyTorch tensor with shape `[batch_size, sequence_length]`\n",
    "- In our case: `[1, 5]` means 1 sequence with 5 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-md",
   "metadata": {},
   "source": [
    "## Step 4: Model Processing\n",
    "\n",
    "Now the model processes these IDs through its neural network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "model-processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([1, 23, 50257])\n",
      "\n",
      "  Batch size: 1\n",
      "  Sequence length: 23 (predictions for each position)\n",
      "  Vocabulary size: 50,257 (scores for each possible token)\n"
     ]
    }
   ],
   "source": [
    "# Run the model on our input\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(model_input_ids)\n",
    "\n",
    "# The model outputs logits (unnormalized probabilities) for each possible next token\n",
    "logits = outputs.logits\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\\n\")\n",
    "print(f\"  Batch size: {logits.shape[0]}\")\n",
    "print(f\"  Sequence length: {logits.shape[1]} (predictions for each position)\")\n",
    "print(f\"  Vocabulary size: {logits.shape[2]:,} (scores for each possible token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964fe49",
   "metadata": {},
   "source": [
    "### Understanding Model Processing\n",
    "\n",
    "Inside the model, here's the processing pipeline:\n",
    "\n",
    "1. **Embedding layer** - Converts each token ID into a dense vector representation\n",
    "2. **Position embeddings** - Adds information about each token's position in the sequence\n",
    "3. **Transformer layers** - Process the sequence through multiple attention and feed-forward layers:\n",
    "   - **Self-attention** - Determines which tokens should pay attention to each other\n",
    "   - **Feed-forward networks** - Process the attended information\n",
    "4. **Output layer** - Generates scores (logits) for every possible next token\n",
    "\n",
    "The output is a tensor of logits - raw scores for each of the 50,257 tokens in the vocabulary. Higher scores indicate the model thinks that token is more likely to come next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-md",
   "metadata": {},
   "source": [
    "## Step 5: Next Token Prediction\n",
    "\n",
    "Now let's look at the model's prediction for the next token after our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "next-token-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predictions for next token:\n",
      "\n",
      "Rank   Token           ID       Probability \n",
      "==================================================\n",
      "1      ' the'          262       28.45%\n",
      "2      ' human'        1692       4.48%\n",
      "3      ' our'          674        3.55%\n",
      "4      ' into'         656        2.90%\n",
      "5      ' people'       661        2.79%\n",
      "6      ' society'      3592       2.19%\n",
      "7      ' a'            257        1.80%\n",
      "8      ' itself'       2346       1.74%\n",
      "9      ' technology'   3037       1.42%\n",
      "10     ' humans'       5384       1.32%\n"
     ]
    }
   ],
   "source": [
    "# We want the predictions for the last position (after \"transforming\")\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "# Get the top 10 most likely tokens\n",
    "top_k = 10\n",
    "topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "# Convert to numpy for easier handling\n",
    "topk_probs = topk_probs.detach().numpy()\n",
    "topk_indices = topk_indices.detach().numpy()\n",
    "\n",
    "# Decode the tokens\n",
    "topk_tokens = [tokenizer.decode([idx]) for idx in topk_indices]\n",
    "\n",
    "print(\"Top 10 predictions for next token:\\n\")\n",
    "print(f\"{'Rank':<6} {'Token':<15} {'ID':<8} {'Probability':<12}\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(top_k):\n",
    "    print(f\"{i+1:<6} {repr(topk_tokens[i]):<15} {topk_indices[i]:<8} {topk_probs[i]*100:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7957c",
   "metadata": {},
   "source": [
    "### Understanding Next Token Prediction\n",
    "\n",
    "The model has analyzed \"Artificial intelligence is transforming\" and predicted what comes next:\n",
    "\n",
    "1. **Raw logits** - The model outputs raw scores for every token in the vocabulary\n",
    "2. **Softmax conversion** - We convert logits to probabilities (they sum to 100%)\n",
    "3. **Top-k selection** - We examine only the most likely candidates\n",
    "\n",
    "Notice how the top token (\" the\") has ~27% probability - the model is fairly confident but not certain. This probability distribution reflects patterns the model learned from its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-md",
   "metadata": {},
   "source": [
    "## Step 6: Token Selection\n",
    "\n",
    "Now we need to select which token to use next. Let's look at different ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "token-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Selection Comparison:\n",
      "\n",
      "  Greedy: ' the'          (always picks most likely)\n",
      "  Sampled: ' the'          (random selection from top candidates)\n",
      "\n",
      "Top-5 tokens with temperature=0.7:\n",
      "\n",
      "Token           Original %    Adjusted %   \n",
      "=============================================\n",
      "' the'           28.45%        83.55%\n",
      "' human'          4.48%         5.96%\n",
      "' our'            3.55%         4.28%\n",
      "' into'           2.90%         3.20%\n",
      "' people'         2.79%         3.02%\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Greedy selection (always pick the most likely token)\n",
    "greedy_index = torch.argmax(next_token_probs).item()\n",
    "greedy_token = tokenizer.decode([greedy_index])\n",
    "\n",
    "# Method 2: Temperature sampling (adjust probability distribution)\n",
    "temperature = 0.7  # Lower = more deterministic, Higher = more random\n",
    "temp_logits = next_token_logits / temperature\n",
    "temp_probs = torch.softmax(temp_logits, dim=0)\n",
    "\n",
    "# Method 3: Top-k sampling (sample from k most likely tokens)\n",
    "k = 5\n",
    "topk_temp_probs, topk_temp_indices = torch.topk(temp_probs, k)\n",
    "topk_temp_probs = topk_temp_probs / topk_temp_probs.sum()  # Renormalize\n",
    "\n",
    "# Sample using temperature + top-k\n",
    "sample_index = np.random.choice(topk_temp_indices.detach().numpy(), \n",
    "                                p=topk_temp_probs.detach().numpy())\n",
    "sample_token = tokenizer.decode([sample_index])\n",
    "\n",
    "print(\"Token Selection Comparison:\\n\")\n",
    "print(f\"  Greedy: {repr(greedy_token):<15} (always picks most likely)\")\n",
    "print(f\"  Sampled: {repr(sample_token):<15} (random selection from top candidates)\\n\")\n",
    "\n",
    "# Show the top-k tokens with temperature adjustment\n",
    "print(f\"Top-{k} tokens with temperature={temperature}:\\n\")\n",
    "print(f\"{'Token':<15} {'Original %':<13} {'Adjusted %':<13}\")\n",
    "print(\"=\" * 45)\n",
    "for i in range(k):\n",
    "    token_id = topk_temp_indices[i].item()\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    orig_prob = next_token_probs[token_id].item() * 100\n",
    "    adj_prob = topk_temp_probs[i].item() * 100\n",
    "    print(f\"{repr(token_text):<15} {orig_prob:>6.2f}%       {adj_prob:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24570a8",
   "metadata": {},
   "source": [
    "### Understanding Token Selection Strategies\n",
    "\n",
    "Two main approaches for selecting the next token:\n",
    "\n",
    "**1. Greedy Selection**\n",
    "- Always picks the token with highest probability\n",
    "- Deterministic - same input always produces same output\n",
    "- Can be repetitive and boring\n",
    "\n",
    "**2. Sampling with Temperature and Top-k**\n",
    "- Introduces controlled randomness\n",
    "- **Temperature** - Adjusts how confident the model is:\n",
    "  - Lower (< 1.0) - Makes high-probability tokens even more likely\n",
    "  - Higher (> 1.0) - Flattens distribution, makes lower-probability tokens more likely\n",
    "- **Top-k** - Only considers the k most likely tokens\n",
    "- **Result** - More varied and interesting outputs\n",
    "\n",
    "The temperature adjustment reshapes the probability distribution before sampling, giving us control over the creativity vs. consistency tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-md",
   "metadata": {},
   "source": [
    "## Step 7: Building the Response\n",
    "\n",
    "Now we'll see the complete text generation process in action, adding one token at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "build-response",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: 'Artificial intelligence (الذكاء الاصطناعي) is transforming'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 1: Generating token 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 candidates:\n",
      "  1. ' the'          (Probability:  68.04%)\n",
      "  2. ' human'        (Probability:   4.85%)\n",
      "  3. ' our'          (Probability:   3.48%)\n",
      "  4. ' into'         (Probability:   2.60%)\n",
      "  5. ' people'       (Probability:   2.46%)\n",
      "\n",
      "✓ Selected: ' the'\n",
      "  Text so far: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the'\n",
      "\n",
      "Step 2: Generating token 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 candidates:\n",
      "  1. ' world'        (Probability:  56.97%)\n",
      "  2. ' way'          (Probability:   9.03%)\n",
      "  3. ' lives'        (Probability:   6.11%)\n",
      "  4. ' human'        (Probability:   3.69%)\n",
      "  5. ' entire'       (Probability:   1.63%)\n",
      "\n",
      "✓ Selected: ' human'\n",
      "  Text so far: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the human'\n",
      "\n",
      "Step 3: Generating token 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 candidates:\n",
      "  1. ' mind'         (Probability:  59.33%)\n",
      "  2. ' brain'        (Probability:  17.61%)\n",
      "  3. ' psyche'       (Probability:   7.21%)\n",
      "  4. ' race'         (Probability:   6.47%)\n",
      "  5. ' body'         (Probability:   2.80%)\n",
      "\n",
      "✓ Selected: ' mind'\n",
      "  Text so far: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the human mind'\n",
      "\n",
      "Step 4: Generating token 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 candidates:\n",
      "  1. ' into'         (Probability:  39.69%)\n",
      "  2. ' and'          (Probability:  22.01%)\n",
      "  3. '.'             (Probability:  14.85%)\n",
      "  4. ' to'           (Probability:   9.24%)\n",
      "  5. ','             (Probability:   7.20%)\n",
      "\n",
      "✓ Selected: ' into'\n",
      "  Text so far: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the human mind into'\n",
      "\n",
      "Step 5: Generating token 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 candidates:\n",
      "  1. ' a'            (Probability:  82.47%)\n",
      "  2. ' an'           (Probability:  11.55%)\n",
      "  3. ' the'          (Probability:   3.78%)\n",
      "  4. ' something'    (Probability:   1.00%)\n",
      "  5. ' one'          (Probability:   0.55%)\n",
      "\n",
      "✓ Selected: ' a'\n",
      "  Text so far: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the human mind into a'\n",
      "\n",
      "================================================================================\n",
      "✓ Final text: 'Artificial intelligence (الذكاء الاصطناعي) is transforming the human mind into a'\n"
     ]
    }
   ],
   "source": [
    "def generate_step_by_step(prompt, max_new_tokens=5, temperature=0.7, top_k=5):\n",
    "    \"\"\"Generate text token by token with detailed output at each step\"\"\"\n",
    "    current_text = prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(f\"Starting prompt: '{prompt}'\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        print(f\"\\nStep {i+1}: Generating token {i+1}/{max_new_tokens}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get next token logits\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get top-k token indices and probabilities\n",
    "        topk_probs, topk_indices = torch.topk(torch.softmax(next_token_logits, dim=0), top_k)\n",
    "        \n",
    "        # Display top candidates\n",
    "        print(f\"\\nTop {top_k} candidates:\")\n",
    "        for j in range(top_k):\n",
    "            token_id = topk_indices[j].item()\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            token_prob = topk_probs[j].item() * 100\n",
    "            print(f\"  {j+1}. {repr(token_text):<15} (Probability: {token_prob:>6.2f}%)\")\n",
    "        \n",
    "        # Renormalize probabilities for top-k\n",
    "        topk_probs = topk_probs / topk_probs.sum()\n",
    "        \n",
    "        # Sample from top-k\n",
    "        chosen_idx = np.random.choice(topk_indices.detach().numpy(), \n",
    "                                      p=topk_probs.detach().numpy())\n",
    "        chosen_token = tokenizer.decode([chosen_idx])\n",
    "        \n",
    "        print(f\"\\n✓ Selected: {repr(chosen_token)}\")\n",
    "        \n",
    "        # Update for next iteration\n",
    "        next_token = torch.tensor([[chosen_idx]])\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        current_text += chosen_token\n",
    "        \n",
    "        print(f\"  Text so far: '{current_text}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✓ Final text: '{current_text}'\")\n",
    "    return current_text\n",
    "\n",
    "# Generate text step by step\n",
    "final_text = generate_step_by_step(prompt, max_new_tokens=5, temperature=0.7, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-process-explanation",
   "metadata": {},
   "source": [
    "### Understanding the Auto-regressive Process\n",
    "\n",
    "What we just witnessed is **auto-regressive generation** - the key process behind LLM text generation:\n",
    "\n",
    "1. **Start with prompt** - Begin with the initial text\n",
    "2. **For each new token:**\n",
    "   - Process all previous tokens through the model\n",
    "   - Get probability distribution for next token\n",
    "   - Apply temperature and top-k filtering\n",
    "   - Sample a token from the filtered distribution\n",
    "   - Append the selected token to our text\n",
    "3. **Repeat** - Continue until reaching desired length\n",
    "\n",
    "**Key insight:** Each new token depends on *all* the tokens that came before it. This is why the model can maintain context and coherence throughout the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameters-effect",
   "metadata": {},
   "source": [
    "## The Effect of Generation Parameters\n",
    "\n",
    "Different parameters can dramatically change the output. Let's experiment with a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parameter-experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Generation Parameters:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Greedy (deterministic)\n",
      "Parameters: {'do_sample': False}\n",
      "Generated: ' the world into a world where people can be educated, educated, and educated'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Low temperature (focused)\n",
      "Parameters: {'temperature': 0.3, 'do_sample': True}\n",
      "Generated: ' the world into a virtual reality world.\\n\\n\\n\\n\\n\\n\\n'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "High temperature (creative)\n",
      "Parameters: {'temperature': 1.5, 'do_sample': True}\n",
      "Generated: \" intelligence with new skills. It's the next generation from this generation from its\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top-k sampling\n",
      "Parameters: {'top_k': 5, 'do_sample': True}\n",
      "Generated: ' the entire world into a virtual reality.\\nThe artificial intelligence (AI),'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top-p / Nucleus sampling\n",
      "Parameters: {'top_p': 0.9, 'do_sample': True}\n",
      "Generated: ' the social consciousness of people in a number of ways. While the United States'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Balanced (recommended)\n",
      "Parameters: {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}\n",
      "Generated: ' the world into a full-fledged technology. The artificial intelligence will be able'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Note: Newline characters (\\n) are tokens the model can generate.\n",
      "Lower temperatures and greedy decoding may generate more newlines as they\n",
      "follow the most common patterns seen in training data (like paragraph breaks).\n"
     ]
    }
   ],
   "source": [
    "def generate_with_params(prompt, max_new_tokens=15, **params):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    # Encode with attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    \n",
    "    # Generate output\n",
    "    output_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=len(inputs.input_ids[0]) + max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Different parameter configurations to try\n",
    "configs = [\n",
    "    {'name': 'Greedy (deterministic)', \n",
    "     'params': {'do_sample': False}},\n",
    "    \n",
    "    {'name': 'Low temperature (focused)', \n",
    "     'params': {'temperature': 0.3, 'do_sample': True}},\n",
    "    \n",
    "    {'name': 'High temperature (creative)', \n",
    "     'params': {'temperature': 1.5, 'do_sample': True}},\n",
    "    \n",
    "    {'name': 'Top-k sampling', \n",
    "     'params': {'top_k': 5, 'do_sample': True}},\n",
    "    \n",
    "    {'name': 'Top-p / Nucleus sampling', \n",
    "     'params': {'top_p': 0.9, 'do_sample': True}},\n",
    "    \n",
    "    {'name': 'Balanced (recommended)', \n",
    "     'params': {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}}\n",
    "]\n",
    "\n",
    "# Generate and display results\n",
    "print(\"Comparing Generation Parameters:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in configs:\n",
    "    output = generate_with_params(prompt, **config['params'])\n",
    "    generated_part = output[len(prompt):]\n",
    "    \n",
    "    print(f\"\\n{config['name']}\")\n",
    "    print(f\"Parameters: {config['params']}\")\n",
    "    # Display with visible newlines to show what the model actually generated\n",
    "    print(f\"Generated: {repr(generated_part)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nNote: Newline characters (\\\\n) are tokens the model can generate.\")\n",
    "print(\"Lower temperatures and greedy decoding may generate more newlines as they\")\n",
    "print(\"follow the most common patterns seen in training data (like paragraph breaks).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameters-explanation",
   "metadata": {},
   "source": [
    "### Generation Parameters Explained\n",
    "\n",
    "**do_sample** (True/False)\n",
    "- `False` - Greedy decoding (always picks most likely token)\n",
    "- `True` - Samples from probability distribution (introduces randomness)\n",
    "\n",
    "**temperature** (0.1 to 2.0+)\n",
    "- Lower (0.3) - More focused, deterministic, repetitive\n",
    "- Default (1.0) - Original probability distribution\n",
    "- Higher (1.5) - More random, creative, potentially incoherent\n",
    "\n",
    "**top_k** (integer, e.g., 5-100)\n",
    "- Limits selection to k most likely tokens\n",
    "- Lower k - More focused on top choices\n",
    "- Higher k - Allows more diversity\n",
    "\n",
    "**top_p / nucleus sampling** (0.0 to 1.0)\n",
    "- Selects from smallest set of tokens whose cumulative probability ≥ p\n",
    "- Adapts based on model confidence\n",
    "- Common values: 0.9 or 0.95\n",
    "\n",
    "**Recommended:** Combine temperature (0.7-0.8) + top_k (40-50) + top_p (0.9) for balanced, high-quality generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-pipeline",
   "metadata": {},
   "source": [
    "## The Complete LLM Text Generation Pipeline\n",
    "\n",
    "Let's summarize the entire pipeline we've explored:\n",
    "\n",
    "### Pipeline Steps\n",
    "\n",
    "1. **Input Text** - Start with a text prompt\n",
    "   \n",
    "2. **Tokenization** - Break text into tokens (words, subwords, or characters)\n",
    "   \n",
    "3. **Token → ID Conversion** - Map each token to its vocabulary ID\n",
    "   \n",
    "4. **Model Processing** - Process IDs through neural network:\n",
    "   - Embedding lookup (convert IDs to vectors)\n",
    "   - Add position information\n",
    "   - Multiple transformer layers (attention + feed-forward)\n",
    "   - Output layer generates logits\n",
    "   \n",
    "5. **Next Token Prediction** - Convert logits to probabilities via softmax\n",
    "   \n",
    "6. **Token Selection** - Choose next token:\n",
    "   - Greedy: Pick most likely\n",
    "   - Sampling: Random selection with temperature/top-k/top-p\n",
    "   \n",
    "7. **Append Token** - Add selected token to output\n",
    "   \n",
    "8. **Repeat** - Continue from step 3 with updated text until reaching desired length\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Auto-regressive** - Each token depends on all previous tokens\n",
    "- **Probabilistic** - Sampling introduces randomness (except greedy mode)\n",
    "- **Iterative** - Generates one token at a time\n",
    "- **Configurable** - Parameters control output quality and diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've explored the complete text generation pipeline of a local LLM, examining each step from input text to final output. \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Tokenization is fundamental** - Text must be converted to tokens, then IDs\n",
    "2. **The model processes sequences** - Not individual words, but entire contexts\n",
    "3. **Logits → Probabilities** - Softmax converts raw scores to interpretable probabilities\n",
    "4. **Selection strategies matter** - Greedy vs. sampling dramatically affects output quality\n",
    "5. **Parameters provide control** - Temperature, top-k, and top-p let you tune generation\n",
    "6. **Auto-regressive generation** - Each token builds on all previous tokens\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "Understanding this pipeline helps you:\n",
    "- **Debug issues** - Identify where problems occur in generation\n",
    "- **Optimize performance** - Choose appropriate parameters for your use case\n",
    "- **Design better prompts** - Understand how context influences predictions\n",
    "- **Build applications** - Integrate LLMs effectively into your systems\n",
    "\n",
    "The probabilistic nature of token selection explains why the same prompt can produce different outputs - a fundamental characteristic of working with LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentals-of-ai-engineering-principles-and-practical-applications-6026542",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
