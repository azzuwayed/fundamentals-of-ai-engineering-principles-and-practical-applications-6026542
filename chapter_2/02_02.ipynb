{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ae6c59",
   "metadata": {},
   "source": [
    "# Running Inference Locally\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized AI applications, but they don't always need to be accessed through cloud APIs. In this lesson, we'll explore how to download, save, and run LLMs locally in your development environment.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Download and save LLMs for offline use\n",
    "- Load models from local storage\n",
    "- Generate text with various parameters\n",
    "- Understand the tradeoffs between local and cloud-based inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup: Install Required Libraries\n",
    "\n",
    "# Install necessary libraries (if not already installed)\n",
    "# - Use copy link mode in containers to avoid hardlink warnings\n",
    "# - Install PyTorch CPU build compatible with your Python\n",
    "%env UV_LINK_MODE=copy\n",
    "!uv pip install --upgrade pip\n",
    "!uv pip install --extra-index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio\n",
    "!uv pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7s0dp09tme",
   "metadata": {},
   "source": [
    "## Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lol1xxfamd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of installed packages\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    print(\"✓ PyTorch version:\", torch.__version__)\n",
    "    print(\"✓ CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"✓ Transformers version:\", transformers.__version__)\n",
    "    print(\"\\nSetup complete! You're ready to work with local LLMs.\")\n",
    "except Exception as e:\n",
    "    print(\"✗ Error during verification:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a6fe5",
   "metadata": {},
   "source": [
    "## Understanding Local LLMs\n",
    "\n",
    "Running LLMs locally offers several advantages:\n",
    "- **Privacy**: Your data doesn't leave your environment\n",
    "- **Cost**: No per-token API charges\n",
    "- **Latency**: No network delays for inference\n",
    "- **Customization**: Full control over model parameters\n",
    "- **Offline capability**: Works without internet connection\n",
    "\n",
    "However, local LLMs also have limitations:\n",
    "- **Hardware requirements**: Models need sufficient RAM and compute resources\n",
    "- **Model size**: Smaller models fit locally but may have reduced capabilities\n",
    "- **Updates**: You manage model versions yourself\n",
    "- **Initial download**: First-time setup requires downloading the model\n",
    "\n",
    "### Downloading a Model\n",
    "\n",
    "Let's start by downloading **DistilGPT2**, a distilled version of GPT-2 that's lightweight and perfect for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Set the directory where you want to save the model\n",
    "save_directory = \"./downloaded_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Download model and tokenizer from Hugging Face Hub\n",
    "print(\"Downloading model from Hugging Face Hub...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model size on disk: ~{model.num_parameters() * 4 / (1024 * 1024):.2f} MB (estimated)\")\n",
    "\n",
    "# Save the model and tokenizer to the specified directory\n",
    "print(f\"\\nSaving model to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5d342",
   "metadata": {},
   "source": [
    "## Loading and Using a Local Model\n",
    "\n",
    "Once saved, we can load the model from local storage instead of downloading it again. This is especially useful for larger models or when working in environments with limited internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from local directory instead of downloading again\n",
    "print(\"Loading model from local directory...\")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Set pad token to avoid warnings during generation\n",
    "local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully from local directory!\")\n",
    "\n",
    "# Set device (CPU for most codespace environments)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ac5b2",
   "metadata": {},
   "source": [
    "## Generating Text with Your Local LLM\n",
    "\n",
    "Now let's create a text generation function with customizable parameters. Understanding these parameters is crucial for controlling the model's output:\n",
    "\n",
    "- **Temperature**: Controls randomness (higher = more creative, lower = more deterministic)\n",
    "  - `< 1.0`: More focused and deterministic\n",
    "  - `= 1.0`: Default randomness\n",
    "  - `> 1.0`: More creative and unpredictable\n",
    "\n",
    "- **Max length**: The maximum number of tokens to generate (including the prompt)\n",
    "\n",
    "- **Top-p (nucleus sampling)**: Limits token selection to the top tokens whose cumulative probability exceeds p\n",
    "  - Range: 0.0 to 1.0\n",
    "  - Lower values = more focused responses\n",
    "\n",
    "- **Top-k**: Limits selection to the k most likely tokens at each step\n",
    "  - Common values: 20-100\n",
    "\n",
    "- **do_sample**: Whether to use sampling (True) or greedy decoding (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe6b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text generation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_text(prompt,\n",
    "                  max_length=50,\n",
    "                  temperature=0.8,\n",
    "                  top_p=0.9,\n",
    "                  top_k=50,\n",
    "                  do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt with customizable parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input text to continue\n",
    "        max_length (int): Maximum length of generated text (including prompt)\n",
    "        temperature (float): Controls randomness (>1.0 = more random, <1.0 = more deterministic)\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        top_k (int): Limits selection to k most likely tokens\n",
    "        do_sample (bool): If False, uses greedy decoding instead of sampling\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text including the prompt\n",
    "    \"\"\"\n",
    "    # Prepare the inputs\n",
    "    inputs = local_tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = local_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        top_k=top_k if do_sample else None,\n",
    "        pad_token_id=local_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and clean the output\n",
    "    generated_text = local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up excess whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', generated_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "print(\"✓ Text generation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c9b3f",
   "metadata": {},
   "source": [
    "## Experimenting with Different Generation Parameters\n",
    "\n",
    "Let's see how different parameters affect the model's output. We'll use the same prompt with varying settings to observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d410e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Example 1: Default parameters (temperature=0.8)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, max_length=75)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 2: Low temperature (more deterministic)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=0.2, max_length=75)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 3: High temperature (more creative/random)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=1.5, max_length=75)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Example 4: Greedy decoding (no sampling)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, do_sample=False, max_length=75)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd59bc7",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "**Temperature Effects:**\n",
    "- Low temperature (0.2): Produces more consistent, predictable outputs\n",
    "- High temperature (1.5): Creates more varied and creative (sometimes incoherent) outputs\n",
    "- Greedy decoding: Always selects the most likely token, producing deterministic results\n",
    "\n",
    "**Best Practices:**\n",
    "- Use low temperature (0.2-0.5) for factual or structured tasks\n",
    "- Use medium temperature (0.7-1.0) for balanced creativity\n",
    "- Use high temperature (1.0-1.5+) for creative writing or brainstorming\n",
    "- Use greedy decoding when you need reproducible outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentals-of-ai-engineering-principles-and-practical-applications-6026542",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
