{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparing Embedding Models\n\nIn this notebook, we'll compare different embedding models to understand their trade-offs in terms of performance, speed, and resource requirements.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand key differences between embedding models\n- Compare models based on accuracy, speed, and resource requirements\n- Select appropriate models for different use cases\n- Measure model performance using contrast metrics\n- Make informed trade-offs between accuracy and efficiency\n\n## What Makes Embedding Models Different?\n\nEmbedding models vary across several dimensions:\n\n1. **Size and computational requirements** - Larger models are more accurate but slower\n2. **Language support** - Monolingual (English) vs. multilingual (50+ languages)\n3. **Context length** - Maximum input text length\n4. **Embedding dimensionality** - Higher dimensions capture more nuance (384 vs. 768)\n5. **Task specialization** - General purpose vs. optimized for specific tasks\n\n## Models We'll Compare\n\n| Model | Dimensions | Specialization |\n|-------|------------|----------------|\n| `all-MiniLM-L6-v2` | 384 | Compact, efficient, general purpose |\n| `all-MiniLM-L12-v2` | 384 | Medium size, better accuracy |\n| `all-mpnet-base-v2` | 768 | Large, highest quality embeddings |\n| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | Multilingual (50+ languages) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['UV_LINK_MODE'] = 'copy'\n\n# Install the required packages\n!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2\n\nprint(\"✓ Required libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport random\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer, util\nimport time\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nprint(\"✓ Libraries imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set visualization style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams.update({'font.size': 11})\n\nprint(\"✓ Visualization settings configured!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Setup: Define Models to Compare\n\nWe'll benchmark four different embedding models with varying characteristics:\n\n# Models to evaluate\nmodels = [\n    'all-MiniLM-L6-v2',                      # Small, fast (384d)\n    'all-MiniLM-L12-v2',                     # Medium size (384d)\n    'all-mpnet-base-v2',                     # Large, powerful (768d)\n    'paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual (384d)\n]\n\nprint(\"✓ Models configured for benchmarking!\")\nprint(f\"\\nTotal models to evaluate: {len(models)}\")\nfor i, model in enumerate(models, 1):\n    print(f\"  {i}. {model}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Create Test Dataset\n\nWe'll create sentence pairs to test model performance:\n- **Similar pairs** - Sentences on the same topic (should have high similarity)\n- **Dissimilar pairs** - Sentences on different topics (should have low similarity)\n\n# Semantically similar sentence pairs organized by topic\nsentence_pairs = [\n    # Technology\n    [\"Machine learning models require significant computational resources.\",\n     \"AI systems need a lot of computing power to train.\"],\n    \n    # Programming\n    [\"What's the best algorithm for text classification?\",\n     \"How can I optimize my neural network training time?\"],\n    \n    # Weather\n    [\"The weather forecast predicts rain tomorrow.\",\n     \"It's going to be wet outside tomorrow according to meteorologists.\"],\n    \n    # Office\n    [\"I need to get a new computer for my office.\",\n     \"My workplace needs updated computing equipment.\"],\n    \n    # Health\n    [\"Regular exercise improves cardiovascular health.\",\n     \"Working out frequently is good for your heart.\"],\n    \n    # Food\n    [\"The restaurant serves authentic Italian pasta dishes.\",\n     \"You can get genuine Italian noodle recipes at that dining place.\"]\n]\n\nprint(\"✓ Similar sentence pairs created!\")\nprint(f\"  Total similar pairs: {len(sentence_pairs)}\")\nprint(f\"  Topics: Technology, Programming, Weather, Office, Health, Food\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate dissimilar pairs by mixing sentences from different topics\ndissimilar_pairs = []\nfor i in range(len(sentence_pairs)):\n    for j in range(len(sentence_pairs)):\n        if i != j:  # Different topics\n            dissimilar_pairs.append([sentence_pairs[i][0], sentence_pairs[j][0]])\n\n# Sample 6 dissimilar pairs to match similar pairs count\nrandom.seed(42)  # For reproducibility\ndissimilar_pairs = random.sample(dissimilar_pairs, 6)\n\n# Create list of all unique sentences for encoding\nall_sentences = []\nfor pair in sentence_pairs + dissimilar_pairs:\n    all_sentences.extend(pair)\nall_sentences = list(set(all_sentences))  # Remove duplicates\n\nprint(\"✓ Dissimilar sentence pairs generated!\")\nprint(f\"  Total dissimilar pairs: {len(dissimilar_pairs)}\")\nprint(f\"  Total unique sentences: {len(all_sentences)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Define Evaluation Function\n\nThis function measures model performance across multiple dimensions:\n\ndef evaluate_model(model_name, all_sentences, sentence_pairs, dissimilar_pairs):\n    \"\"\"\n    Evaluate an embedding model on similar and dissimilar sentence pairs.\n    \n    Returns metrics:\n    - Model size and dimensions\n    - Loading and encoding time\n    - Similarity scores for similar/dissimilar pairs\n    - Contrast score (difference between similar and dissimilar)\n    \"\"\"\n    start_time = time.time()\n    \n    # Load model\n    model_load_time = time.time()\n    model = SentenceTransformer(model_name)\n    model_load_time = time.time() - model_load_time\n    \n    # Encode all sentences at once (efficient batch processing)\n    encoding_time = time.time()\n    embeddings_dict = {sentence: model.encode(sentence) for sentence in all_sentences}\n    encoding_time = time.time() - encoding_time\n    \n    # Get model metadata\n    dim = next(iter(embeddings_dict.values())).shape[0]\n    model_size_mb = sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024\n    \n    # Calculate similarities for similar pairs\n    similar_scores = []\n    for s1, s2 in sentence_pairs:\n        score = util.cos_sim(\n            embeddings_dict[s1].reshape(1, -1),\n            embeddings_dict[s2].reshape(1, -1)\n        ).item()\n        similar_scores.append(score)\n    \n    # Calculate similarities for dissimilar pairs\n    dissimilar_scores = []\n    for s1, s2 in dissimilar_pairs:\n        score = util.cos_sim(\n            embeddings_dict[s1].reshape(1, -1),\n            embeddings_dict[s2].reshape(1, -1)\n        ).item()\n        dissimilar_scores.append(score)\n    \n    # Calculate metrics\n    avg_similar = sum(similar_scores) / len(similar_scores)\n    avg_dissimilar = sum(dissimilar_scores) / len(dissimilar_scores)\n    contrast = avg_similar - avg_dissimilar  # Key metric!\n    \n    total_time = time.time() - start_time\n    \n    return {\n        'Model': model_name,\n        'Dimensions': dim,\n        'Size (MB)': model_size_mb,\n        'Load Time (s)': model_load_time,\n        'Encoding Time (s)': encoding_time,\n        'Total Time (s)': total_time,\n        'Avg Similar Score': avg_similar,\n        'Avg Different Score': avg_dissimilar,\n        'Contrast': contrast,\n        'Similar Scores': similar_scores,\n        'Dissimilar Scores': dissimilar_scores\n    }\n\nprint(\"✓ Evaluation function defined successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Run Model Evaluation\n\nNow let's benchmark all four models:\n\n# Evaluate all models\nresults = []\nprint(\"=\" * 80)\nfor i, model_name in enumerate(models, 1):\n    print(f\"\\n[{i}/{len(models)}] Evaluating {model_name}...\")\n    model_results = evaluate_model(model_name, all_sentences, sentence_pairs, dissimilar_pairs)\n    results.append(model_results)\n    print(f\"✓ Completed {model_name}\")\n    print(f\"  Contrast score: {model_results['Contrast']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ All model evaluations complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Model Comparison Summary\n\nLet's view the key metrics for all models:\n\n# Convert to DataFrame\ndf = pd.DataFrame(results)\n\n# Create display-friendly version\ndisplay_df = df[['Model', 'Dimensions', 'Size (MB)', 'Encoding Time (s)',\n                'Avg Similar Score', 'Avg Different Score', 'Contrast']]\n\nprint(\"=\" * 80)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\" * 80)\nprint(display_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\nprint(\"=\" * 80)\n\nprint(\"\\n✓ Summary table generated!\")\nprint(\"\\nKey metrics:\")\nprint(\"  • Contrast = Avg Similar - Avg Different (higher is better)\")\nprint(\"  • Higher contrast = better at distinguishing relevant from irrelevant\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Detailed Pair Analysis\n\nLet's examine how each model scored individual sentence pairs:\n\nprint(\"=\" * 80)\nprint(\"DETAILED PAIR ANALYSIS\")\nprint(\"=\" * 80)\n\nfor idx, model_data in enumerate(results):\n    print(f\"\\n{'─' * 80}\")\n    print(f\"Model: {model_data['Model']}\")\n    print(f\"{'─' * 80}\")\n    \n    print(\"\\n✓ SIMILAR PAIRS (should have HIGH similarity):\")\n    for i, score in enumerate(model_data['Similar Scores']):\n        s1, s2 = sentence_pairs[i]\n        print(f\"  {i+1}. Score: {score:.4f}\")\n        print(f\"     \\\"{s1[:50]}...\\\"\")\n        print(f\"     \\\"{s2[:50]}...\\\"\")\n    \n    print(\"\\n✗ DISSIMILAR PAIRS (should have LOW similarity):\")\n    for i, score in enumerate(model_data['Dissimilar Scores']):\n        s1, s2 = dissimilar_pairs[i]\n        print(f\"  {i+1}. Score: {score:.4f}\")\n        print(f\"     \\\"{s1[:50]}...\\\"\")\n        print(f\"     \\\"{s2[:50]}...\\\"\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualization 1: Similarity Score Distributions\n\nCompare how each model distributes similarity scores:\n\nplt.figure(figsize=(14, 7))\n\n# Prepare data for boxplot\nboxplot_data = []\nmodel_labels = []\n\nfor model_data in results:\n    boxplot_data.append(model_data['Similar Scores'])\n    boxplot_data.append(model_data['Dissimilar Scores'])\n    # Shorten model names for display\n    short_name = model_data['Model'].replace('paraphrase-', '').replace('-MiniLM-', '-ML')\n    model_labels.append(f\"{short_name}\\nSimilar\")\n    model_labels.append(f\"{short_name}\\nDifferent\")\n\n# Color pattern: green for similar, red for dissimilar\ncolors = ['lightgreen', 'lightcoral'] * len(models)\n\n# Create boxplot\nbp = plt.boxplot(boxplot_data, patch_artist=True, vert=False)\nplt.yticks(range(1, len(model_labels) + 1), model_labels, fontsize=9)\nplt.xlabel('Cosine Similarity Score', fontsize=12)\nplt.title('Distribution of Similarity Scores by Model', fontsize=14, weight='bold')\n\n# Color the boxes\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.7)\n\nplt.grid(alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Boxplot visualization complete!\")\nprint(\"\\nInterpretation:\")\nprint(\"  • Green boxes (similar pairs) should be right-shifted (higher scores)\")\nprint(\"  • Red boxes (dissimilar pairs) should be left-shifted (lower scores)\")\nprint(\"  • Greater separation = better model discrimination\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualization 2: Model Efficiency\n\nCompare model size vs. encoding speed (bubble size = dimensionality):\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(df['Size (MB)'], df['Encoding Time (s)'],\n                     s=df['Dimensions'], alpha=0.6, c=range(len(df)), \n                     cmap='viridis', edgecolors='black', linewidth=2)\n\n# Add model labels\nfor i, model in enumerate(df['Model']):\n    short_name = model.split('-')[1] if '-' in model else model[:10]\n    plt.annotate(short_name, \n                (df['Size (MB)'].iloc[i], df['Encoding Time (s)'].iloc[i]),\n                xytext=(8, 8), textcoords='offset points',\n                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7),\n                fontsize=9, weight='bold')\n\nplt.xlabel('Model Size (MB)', fontsize=12)\nplt.ylabel('Encoding Time (seconds)', fontsize=12)\nplt.title('Model Efficiency Trade-offs\\n(bubble size = embedding dimensions)', \n         fontsize=14, weight='bold')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Efficiency visualization complete!\")\nprint(\"\\nInterpretation:\")\nprint(\"  • Bottom-left = Fast and small (efficient)\")\nprint(\"  • Top-right = Slow and large (resource-intensive)\")\nprint(\"  • Larger bubbles = higher dimensionality (more detailed embeddings)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualization 3: Performance vs. Resources\n\nCompare accuracy (contrast) vs. speed (colored by model size):\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(df['Total Time (s)'], df['Contrast'], \n                     c=df['Size (MB)'], s=250, cmap='plasma', \n                     alpha=0.7, edgecolors='black', linewidth=2)\nplt.colorbar(scatter, label='Model Size (MB)')\n\n# Add model labels\nfor i, model in enumerate(df['Model']):\n    short_name = model.split('-')[1] if '-' in model else model[:10]\n    plt.annotate(short_name,\n                (df['Total Time (s)'].iloc[i], df['Contrast'].iloc[i]),\n                xytext=(8, 8), textcoords='offset points',\n                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7),\n                fontsize=9, weight='bold')\n\nplt.xlabel('Total Processing Time (seconds)', fontsize=12)\nplt.ylabel('Contrast Score (Similar - Dissimilar)', fontsize=12)\nplt.title('Performance vs. Resource Requirements', fontsize=14, weight='bold')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Performance comparison complete!\")\nprint(\"\\nInterpretation:\")\nprint(\"  • Top-left corner = Best (high accuracy, fast processing)\")\nprint(\"  • Bottom-right = Worst (low accuracy, slow processing)\")\nprint(\"  • Color indicates model size (darker = larger model)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've compared four embedding models across multiple dimensions to understand their trade-offs.\n\n### Key Takeaways\n\n1. **Contrast is king** - The contrast score (difference between similar and dissimilar pairs) is the most important metric for distinguishing relevant from irrelevant information\n\n2. **Size vs. Speed trade-off** - Larger models have better semantic understanding but require more resources and run slower\n\n3. **Dimensionality matters** - Higher-dimensional embeddings (768d) capture more nuance than lower-dimensional ones (384d)\n\n4. **Specialization helps** - Models fine-tuned for specific tasks (e.g., multilingual, QA) perform better on those tasks\n\n5. **Test with your data** - These benchmarks use general examples; always test with domain-specific data for best results\n\n### Model Selection Guidelines\n\n**Choose `all-MiniLM-L6-v2` when:**\n- Speed and efficiency are critical\n- Deploying on edge devices or mobile\n- Resource constraints are tight\n- Good enough accuracy for general tasks\n\n**Choose `all-mpnet-base-v2` when:**\n- Maximum accuracy is required\n- Server-side deployment with ample resources\n- Handling complex semantic relationships\n- Performance matters more than speed\n\n**Choose `paraphrase-multilingual` when:**\n- Working with multiple languages (50+)\n- Building international applications\n- Language detection not available\n- Multilingual support is essential\n\n**Choose `all-MiniLM-L12-v2` when:**\n- Balanced performance needed\n- Middle ground between speed and accuracy\n- Moderate resource availability\n- Good general-purpose choice\n\n### Decision Framework\n\nWhen selecting an embedding model, consider:\n\n1. **Accuracy requirements** - How critical is perfect semantic matching?\n2. **Inference speed** - What are your latency constraints?\n3. **Resource constraints** - What's your memory and compute budget?\n4. **Multilingual needs** - Do you need multiple language support?\n5. **Deployment target** - Cloud server, edge device, or mobile?\n6. **Domain specificity** - Is there a specialized model for your use case?\n\nThe best model is the one that meets your requirements with the least resources!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}