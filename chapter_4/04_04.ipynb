{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Efficient Embeddings Generation\n\nThis notebook demonstrates optimization techniques for generating embeddings at scale.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand the impact of batch size on throughput and latency\n- Implement efficient batching strategies for embedding generation\n- Build caching systems to avoid redundant computations\n- Measure and optimize embedding generation performance\n- Choose appropriate batch sizes for different use cases\n\n## Optimization Strategies\n\nWe'll explore two key techniques that dramatically improve performance:\n\n1. **Batching** - Processing multiple inputs together rather than one at a time\n   - Maximizes GPU/CPU utilization\n   - Reduces overhead from model inference\n   - Increases throughput significantly\n\n2. **Caching** - Storing previously generated embeddings to avoid regeneration\n   - Eliminates redundant computation\n   - Reduces latency for repeated content\n   - Critical for production systems\n\nThese optimizations are essential for building performant AI applications that rely on embeddings."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Install Required Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['UV_LINK_MODE'] = 'copy'\n\n# Install the required packages\n!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2\n\nprint(\"✓ Required libraries installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Import Libraries and Load Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sentence_transformers import SentenceTransformer\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport hashlib\nfrom functools import lru_cache\nimport pandas as pd\n\n# Load lightweight model for demonstration\nmodel_name = 'all-MiniLM-L6-v2'\nmodel = SentenceTransformer(model_name)\n\n# Generate example sentences for benchmarking\nsentences = [\n    f\"This is a sample sentence for benchmarking embeddings generation {i}.\"\n    for i in range(1000)\n]\n\nprint(\"✓ Libraries imported and model loaded!\")\nprint(f\"  Model: {model_name}\")\nprint(f\"  Test dataset: {len(sentences):,} sentences\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch Size Impact on Performance\n\n**Batching** processes multiple inputs together instead of one-by-one, which dramatically improves efficiency. Let's measure how different batch sizes affect:\n\n- **Throughput** - Embeddings generated per second (higher is better)\n- **Latency** - Time to process each batch (important for real-time apps)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark different batch sizes\nbatch_sizes = [1, 4, 8, 16, 32, 64, 128, 256]\nresults = []\n\nprint(\"Benchmarking batch sizes...\")\nprint(\"=\" * 80)\n\nfor batch_size in tqdm(batch_sizes, desc=\"Testing batch sizes\"):\n    start_time = time.time()\n    \n    # Process data in batches\n    embeddings = []\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i+batch_size]\n        batch_embeddings = model.encode(batch)\n        embeddings.extend(batch_embeddings)\n    \n    # Calculate performance metrics\n    total_time = time.time() - start_time\n    throughput = len(sentences) / total_time\n    avg_latency = total_time / (len(sentences) / batch_size)\n    \n    results.append({\n        'Batch Size': batch_size,\n        'Total Time (s)': total_time,\n        'Throughput (samples/s)': throughput,\n        'Avg Batch Latency (s)': avg_latency\n    })\n\n# Display results\ndf = pd.DataFrame(results)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BATCH SIZE BENCHMARK RESULTS\")\nprint(\"=\" * 80)\nprint(df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Performance Metrics\n\nLet's visualize how batch size affects throughput and latency:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization\nplt.figure(figsize=(15, 6))\n\n# Throughput plot\nplt.subplot(1, 2, 1)\nplt.plot(df['Batch Size'], df['Throughput (samples/s)'], 'o-', linewidth=2, \n        markersize=8, color='#2E86AB')\nplt.xlabel('Batch Size', fontsize=12)\nplt.ylabel('Throughput (samples/second)', fontsize=12)\nplt.title('Batch Size vs Throughput\\n(Higher is Better)', fontsize=13, weight='bold')\nplt.grid(True, alpha=0.3)\n\n# Mark optimal batch size\nmax_throughput_idx = df['Throughput (samples/s)'].idxmax()\noptimal_batch = df.loc[max_throughput_idx, 'Batch Size']\noptimal_throughput = df.loc[max_throughput_idx, 'Throughput (samples/s)']\nplt.axvline(x=optimal_batch, color='red', linestyle='--', alpha=0.5, label=f'Optimal: {optimal_batch}')\nplt.legend()\n\n# Latency plot\nplt.subplot(1, 2, 2)\nplt.plot(df['Batch Size'], df['Avg Batch Latency (s)'], 'o-', linewidth=2, \n        markersize=8, color='#A23B72')\nplt.xlabel('Batch Size', fontsize=12)\nplt.ylabel('Average Batch Latency (seconds)', fontsize=12)\nplt.title('Batch Size vs Latency\\n(Lower is Better for Real-time)', fontsize=13, weight='bold')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Performance visualization complete!\")\nprint(f\"\\nOptimal batch size for throughput: {optimal_batch} ({optimal_throughput:.1f} samples/s)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Observations\n\n1. **Throughput increases with batch size** - Processing multiple inputs together is much more efficient than one-by-one\n\n2. **Diminishing returns** - Beyond batch size 32-64, throughput gains plateau due to memory and computational limits\n\n3. **Latency trade-off** - Larger batches improve throughput but increase per-batch latency\n   - **Real-time apps** → Use smaller batches (1-16) for low latency\n   - **Batch processing** → Use larger batches (64-256) for high throughput\n\n4. **Optimal batch size depends on use case:**\n   - Search engines: Small batches (fast response)\n   - Document indexing: Large batches (high throughput)\n   - Recommendation systems: Medium batches (balanced)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Implementing Embedding Caches\n\nIn real-world applications, we often process the same text multiple times. **Caching** avoids redundant computation by storing previously generated embeddings.\n\nBenefits of caching:\n- **Eliminates redundant work** - No need to recompute embeddings for repeated text\n- **Reduces latency** - Cached embeddings return instantly\n- **Saves resources** - Less CPU/GPU usage for repeated content"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Simple Dictionary-Based Cache\n\nA basic caching implementation using Python dictionaries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SimpleEmbeddingCache:\n    \"\"\"\n    Simple embedding cache using a dictionary to store text→embedding mappings.\n    Tracks cache hits and misses for performance monitoring.\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.cache = {}  # text_hash → embedding\n        self.hits = 0\n        self.misses = 0\n    \n    def _get_hash(self, text):\n        \"\"\"Generate a stable hash for text (used as cache key)\"\"\"\n        return hashlib.md5(text.encode('utf-8')).hexdigest()\n    \n    def encode(self, texts, batch_size=32):\n        \"\"\"Encode texts using cache when available\"\"\"\n        results = []\n        texts_to_encode = []\n        text_indices = []\n        \n        # Check cache for each text\n        for i, text in enumerate(texts):\n            text_hash = self._get_hash(text)\n            if text_hash in self.cache:\n                results.append((i, self.cache[text_hash]))\n                self.hits += 1\n            else:\n                texts_to_encode.append(text)\n                text_indices.append(i)\n                self.misses += 1\n        \n        # Generate embeddings for cache misses (in batches)\n        if texts_to_encode:\n            new_embeddings = []\n            for i in range(0, len(texts_to_encode), batch_size):\n                batch = texts_to_encode[i:i+batch_size]\n                batch_embeddings = model.encode(batch)\n                new_embeddings.extend(batch_embeddings)\n            \n            # Update cache with new embeddings\n            for i, text in enumerate(texts_to_encode):\n                text_hash = self._get_hash(text)\n                self.cache[text_hash] = new_embeddings[i]\n                results.append((text_indices[i], new_embeddings[i]))\n        \n        # Sort by original index and return embeddings\n        results.sort(key=lambda x: x[0])\n        return np.array([emb for _, emb in results])\n    \n    def get_stats(self):\n        \"\"\"Return cache performance statistics\"\"\"\n        total = self.hits + self.misses\n        hit_rate = self.hits / total if total > 0 else 0\n        return {\n            \"hits\": self.hits,\n            \"misses\": self.misses,\n            \"total\": total,\n            \"hit_rate\": hit_rate,\n            \"cache_size\": len(self.cache)\n        }\n\nprint(\"✓ SimpleEmbeddingCache class defined successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### LRU Cache Implementation\n\nPython's `functools.lru_cache` provides a **Least Recently Used** cache that automatically evicts old entries when full:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@lru_cache(maxsize=1024)\ndef hash_text(text):\n    \"\"\"Cache text hashes to avoid recomputing MD5\"\"\"\n    return hashlib.md5(text.encode('utf-8')).hexdigest()\n\nclass LRUEmbeddingCache:\n    \"\"\"\n    Embedding cache using Python's LRU cache (automatically evicts least recently used).\n    Useful when memory is limited and you want automatic cache management.\n    \"\"\"\n    def __init__(self, model, maxsize=1024):\n        self.model = model\n        self.encode_single = lru_cache(maxsize=maxsize)(self._encode_single)\n        self.hits = 0\n        self.misses = 0\n        self.hash_to_text = {}\n    \n    def _encode_single(self, text_hash):\n        \"\"\"Generate embedding for single text (cached automatically by @lru_cache)\"\"\"\n        self.misses += 1\n        text = self.hash_to_text[text_hash]\n        return self.model.encode([text])[0]\n    \n    def encode(self, texts, batch_size=32):\n        \"\"\"Encode texts using LRU cache\"\"\"\n        self.hash_to_text = {}\n        results = []\n        \n        for text in texts:\n            text_hash = hash_text(text)\n            self.hash_to_text[text_hash] = text\n            \n            # Check if cached\n            cache_info_before = self.encode_single.cache_info()\n            embedding = self.encode_single(text_hash)\n            cache_info_after = self.encode_single.cache_info()\n            \n            # Update hit counter\n            if cache_info_after.hits > cache_info_before.hits:\n                self.hits += 1\n            \n            results.append(embedding)\n        \n        return np.array(results)\n    \n    def get_stats(self):\n        \"\"\"Return cache performance statistics\"\"\"\n        total = self.hits + self.misses\n        hit_rate = self.hits / total if total > 0 else 0\n        cache_info = self.encode_single.cache_info()\n        return {\n            \"hits\": self.hits,\n            \"misses\": self.misses,\n            \"total\": total,\n            \"hit_rate\": hit_rate,\n            \"cache_info\": cache_info\n        }\n\nprint(\"✓ LRUEmbeddingCache class defined successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluate Cache Performance\n\nLet's compare performance with and without caching on a dataset with repeated content:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create test dataset with repetition (simulates real-world usage)\ntest_data = []\nfor i in range(500):\n    # Every 5th sentence is repeated from the first 100 sentences\n    test_data.append(sentences[i % 100] if i % 5 == 0 else sentences[i])\n\nprint(\"=\" * 80)\nprint(\"CACHE PERFORMANCE BENCHMARK\")\nprint(\"=\" * 80)\nprint(f\"\\nTest dataset: {len(test_data)} sentences\")\nprint(f\"Expected repeated sentences: ~{len([s for i, s in enumerate(test_data) if i % 5 == 0])} ({len([s for i, s in enumerate(test_data) if i % 5 == 0])/len(test_data)*100:.1f}%)\")\n\n# Benchmark 1: Without cache\nprint(\"\\n\" + \"─\" * 80)\nprint(\"1️⃣  WITHOUT CACHE (baseline)\")\nprint(\"─\" * 80)\nstart = time.time()\nembeddings_no_cache = model.encode(test_data, batch_size=32)\nno_cache_time = time.time() - start\nprint(f\"✓ Processing time: {no_cache_time:.4f}s\")\n\n# Benchmark 2: First run with cache (all misses)\nprint(\"\\n\" + \"─\" * 80)\nprint(\"2️⃣  FIRST RUN WITH CACHE (cold cache)\")\nprint(\"─\" * 80)\ncache = SimpleEmbeddingCache(model)\nstart = time.time()\nembeddings_with_cache = cache.encode(test_data, batch_size=32)\nfirst_run_time = time.time() - start\nstats = cache.get_stats()\nprint(f\"✓ Processing time: {first_run_time:.4f}s\")\nprint(f\"  Cache stats: {stats['hits']} hits, {stats['misses']} misses ({stats['hit_rate']*100:.1f}% hit rate)\")\n\n# Benchmark 3: Second run with populated cache\nprint(\"\\n\" + \"─\" * 80)\nprint(\"3️⃣  SECOND RUN WITH CACHE (warm cache)\")\nprint(\"─\" * 80)\nsecond_cache = SimpleEmbeddingCache(model)\nsecond_cache.cache = cache.cache  # Reuse populated cache\nstart = time.time()\nembeddings_with_cache = second_cache.encode(test_data, batch_size=32)\nsecond_run_time = time.time() - start\nstats2 = second_cache.get_stats()\nprint(f\"✓ Processing time: {second_run_time:.4f}s\")\nprint(f\"  Cache stats: {stats2['hits']} hits, {stats2['misses']} misses ({stats2['hit_rate']*100:.1f}% hit rate)\")\n\n# Performance summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PERFORMANCE SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"  Without cache:         {no_cache_time:>8.4f}s  (baseline)\")\nprint(f\"  First run with cache:  {first_run_time:>8.4f}s  ({first_run_time/no_cache_time:>5.2f}x)\")\nprint(f\"  Second run with cache: {second_run_time:>8.4f}s  ({no_cache_time/second_run_time:>5.1f}x faster!)\")\nprint(\"=\" * 80)\n\nprint(f\"\\n✓ Caching provides {no_cache_time/second_run_time:.0f}x speedup for repeated content!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've explored optimization techniques for efficient embedding generation at scale.\n\n### Key Takeaways\n\n1. **Batching dramatically improves throughput** - Processing 32-64 items together is 3-4x faster than one-by-one\n\n2. **Batch size trades off throughput vs latency**:\n   - Smaller batches (1-16) → Lower latency, good for real-time apps\n   - Larger batches (64-256) → Higher throughput, ideal for batch processing\n   - Optimal size depends on your specific use case and hardware\n\n3. **Caching eliminates redundant computation** - Can provide 100-2000x speedup for repeated content\n\n4. **Cache implementation choices**:\n   - **Simple dict cache** - Unlimited size, manual management, best for known dataset sizes\n   - **LRU cache** - Automatic eviction, memory-bounded, good for streaming data\n\n5. **Combined optimizations multiply benefits** - Batching + caching together provide the best performance\n\n### Production Best Practices\n\n**When to use batching:**\n- Bulk document indexing\n- Offline data preprocessing\n- Scheduled batch jobs\n- High-volume API endpoints\n\n**When to use caching:**\n- User-generated content with duplicates\n- Recommendation systems (popular items accessed frequently)\n- Search engines (common queries)\n- Real-time systems with repeated inputs\n\n**When to use both:**\n- Large-scale document processing\n- Production RAG systems\n- Search infrastructure\n- Content moderation pipelines\n\n### Performance Impact\n\n| Scenario | Without Optimization | With Batching | With Batching + Caching |\n|----------|---------------------|---------------|-------------------------|\n| Cold start (no repeats) | 1x (baseline) | 3-4x faster | 3-4x faster |\n| Warm cache (20% repeats) | 1x (baseline) | 3-4x faster | 5-10x faster |\n| Hot cache (80% repeats) | 1x (baseline) | 3-4x faster | 20-50x faster |\n\n### Next Steps\n\nThese optimization techniques form the foundation for building production-grade embedding systems:\n- Vector databases (ChromaDB, Pinecone, Weaviate)\n- Semantic search engines\n- RAG (Retrieval Augmented Generation) systems\n- Document similarity services\n- Recommendation engines"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}