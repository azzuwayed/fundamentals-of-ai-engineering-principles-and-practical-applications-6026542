{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Understanding Text Embeddings: From Words to Vectors\n\nThis notebook explores text embeddings, a fundamental concept in natural language processing. We'll investigate how computers understand semantic meaning by converting words and sentences into numerical vectors.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand what text embeddings are and why they're important\n- Generate embeddings using pre-trained models\n- Measure semantic similarity using cosine similarity\n- Visualize high-dimensional embeddings in 2D space\n- Apply embeddings to find semantically similar text\n\n## What Are Embeddings?\n\n**Embeddings** are dense numerical representations of data in a continuous vector space where:\n- Similar meanings are positioned close together \n- Relative positions capture semantic relationships\n- Each dimension captures different aspects of meaning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Install Required Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['UV_LINK_MODE'] = 'copy'\n\n# Install the required packages\n!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2\n\nprint(\"✓ Required libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Set up matplotlib\ntry:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    try:\n        plt.style.use('seaborn-whitegrid')  # Fallback for older versions\n    except:\n        pass  # Default style if neither is available\n        \nplt.rcParams['figure.figsize'] = (10, 7)\nnp.random.seed(42)  # For reproducibility\n\nprint(\"✓ Libraries imported and configured successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Embedding Model\n\nWe'll use the `all-MiniLM-L6-v2` model:\n- Creates 384-dimensional embeddings\n- Optimized for semantic similarity tasks\n- Fast and efficient for most applications"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nprint(f\"✓ Model loaded successfully!\")\nprint(f\"  Model name: all-MiniLM-L6-v2\")\nprint(f\"  Embedding dimensions: {model.get_sentence_embedding_dimension()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create and Examine Embeddings\n\nLet's create embeddings for example sentences grouped by topic to see how the model captures semantic similarity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example sentences grouped by topic\nsentences = [\n    # AI/ML related sentences\n    \"I love machine learning and artificial intelligence.\",\n    \"AI and ML are fascinating fields of study.\",\n    \n    # Weather related sentences\n    \"The weather is beautiful today.\",\n    \"It's a sunny day with clear skies.\",\n    \n    # Python related sentences\n    \"Python is my favorite programming language.\",\n    \"I enjoy coding in Python for data analysis.\"\n]\n\n# Topic labels for visualization\ntopics = ['AI/ML', 'AI/ML', 'Weather', 'Weather', 'Python', 'Python']\n\n# Display our sentences with their topics\nprint(\"Example sentences grouped by topic:\\n\")\nprint(\"=\" * 80)\nfor i, (sentence, topic) in enumerate(zip(sentences, topics)):\n    print(f\"  {i+1}. [{topic:7}] {sentence}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create embeddings for our sentences\nembeddings = model.encode(sentences)\n\nprint(\"✓ Embeddings created successfully!\\n\")\nprint(f\"Embedding information:\")\nprint(f\"  Shape of each embedding: {embeddings[0].shape}\")\nprint(f\"  Number of embeddings: {len(embeddings)}\")\nprint(f\"  Data type: {embeddings[0].dtype}\")\n\n# Show a snippet of the first embedding\nprint(f\"\\nFirst 10 dimensions of embedding #1:\")\nprint(f\"  {embeddings[0][:10]}\")\nprint(f\"\\nEmbedding statistics:\")\nprint(f\"  Min value: {embeddings[0].min():>8.4f}\")\nprint(f\"  Max value: {embeddings[0].max():>8.4f}\")\nprint(f\"  Mean value: {embeddings[0].mean():>8.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Measure Similarity with Cosine Similarity\n\n**Cosine Similarity** measures the cosine of the angle between two vectors:\n- **Range:** -1 (opposite direction) to 1 (identical direction)\n- **Interpretation:** Higher values indicate greater semantic similarity\n- **Formula:** similarity = (A · B) / (||A|| × ||B||)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate cosine similarity between all pairs of embeddings\nsimilarity_matrix = cosine_similarity(embeddings)\n\nprint(\"✓ Cosine similarity matrix calculated!\\n\")\nprint(\"Similarity matrix (6×6):\")\nprint(\"=\" * 80)\n\n# Display with proper formatting\nnp.set_printoptions(precision=4, suppress=True)\nprint(similarity_matrix)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nKey observations:\")\nprint(\"  • Diagonal values = 1.0 (each sentence is identical to itself)\")\nprint(\"  • High similarity (>0.6) between sentences on the same topic\")\nprint(\"  • Low similarity (<0.1) between sentences on different topics\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create labels for our heatmap\nlabels = [f\"S{i+1}: {topic}\" for i, topic in enumerate(topics)]\n\n# Create a heatmap of the similarity matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(similarity_matrix, \n            annot=True, \n            fmt='.3f',\n            cmap='viridis', \n            xticklabels=labels, \n            yticklabels=labels,\n            cbar_kws={'label': 'Cosine Similarity'})\nplt.title('Cosine Similarity Heatmap', fontsize=14, weight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✓ Heatmap visualization complete!\")\nprint(\"\\nHeatmap interpretation:\")\nprint(\"  • Diagonal (1.0 values) → Each sentence compared with itself\")\nprint(\"  • Bright blocks → High similarity between sentences on the same topic\")\nprint(\"  • Dark areas → Low similarity between sentences on different topics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Embeddings in 2D Space\n\nWe'll use **PCA (Principal Component Analysis)** to reduce our 384-dimensional embeddings to 2D for visualization while preserving as much variance as possible."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reduce embeddings to 2 dimensions using PCA\npca = PCA(n_components=2)\nembeddings_2d = pca.fit_transform(embeddings)\n\n# Set up colors for topics\ntopic_colors = {'AI/ML': 'red', 'Weather': 'blue', 'Python': 'green'}\ncolors = [topic_colors[topic] for topic in topics]\n\n# Plot the 2D embeddings\nplt.figure(figsize=(12, 8))\nfor i, (x, y) in enumerate(embeddings_2d):\n    plt.scatter(x, y, c=colors[i], s=150, alpha=0.7, edgecolors='black', linewidth=2)\n    plt.annotate(f\"S{i+1}\", \n                xy=(x, y), \n                xytext=(5, 5), \n                textcoords='offset points',\n                fontsize=12,\n                weight='bold')\n\n# Add a legend\nfor topic, color in topic_colors.items():\n    plt.scatter([], [], c=color, label=topic, s=150, alpha=0.7, edgecolors='black', linewidth=2)\nplt.legend(loc='upper right', fontsize=11)\n\n# Add title and labels\nplt.title('2D PCA Projection of Sentence Embeddings', fontsize=15, weight='bold')\nplt.xlabel(f'Principal Component 1 (Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=12)\nplt.ylabel(f'Principal Component 2 (Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=12)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ 2D visualization complete!\\n\")\nprint(f\"PCA results:\")\nprint(f\"  Total variance captured: {sum(pca.explained_variance_ratio_):.2%}\")\nprint(f\"  PC1 variance: {pca.explained_variance_ratio_[0]:.2%}\")\nprint(f\"  PC2 variance: {pca.explained_variance_ratio_[1]:.2%}\")\nprint(f\"\\nNotice how sentences on the same topic cluster together in 2D space!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test with New Sentences\n\nLet's test how the model handles new sentences and finds their semantic matches among our original sentences."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define new sentences\nnew_sentences = [\n    \"Deep learning has revolutionized computer vision.\",  # AI/ML related\n    \"The forecast predicts rain for tomorrow.\",           # Weather related\n    \"NumPy and Pandas are essential Python libraries.\"    # Python related\n]\n\n# Create embeddings for the new sentences\nnew_embeddings = model.encode(new_sentences)\n\nprint(\"✓ New sentence embeddings created!\\n\")\nprint(\"=\" * 80)\n\n# Calculate similarity between new and original sentences\nsimilarity_to_original = cosine_similarity(new_embeddings, embeddings)\n\n# Find the most similar original sentence for each new sentence\nfor i, new_sent in enumerate(new_sentences):\n    most_similar_idx = np.argmax(similarity_to_original[i])\n    similarity_score = similarity_to_original[i][most_similar_idx]\n    \n    print(f\"\\nNew sentence #{i+1}:\")\n    print(f\"  \\\"{new_sent}\\\"\")\n    print(f\"\\nMost similar original sentence:\")\n    print(f\"  \\\"{sentences[most_similar_idx]}\\\"\")\n    print(f\"\\nSimilarity score: {similarity_score:.4f}\")\n    print(f\"Topic match: {topics[most_similar_idx]}\")\n    print(\"-\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Original and New Sentences Together\n\nLet's see how the new sentences position themselves relative to the original ones in 2D space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Combine original and new embeddings\nall_embeddings = np.vstack([embeddings, new_embeddings])\nall_topics = topics + ['AI/ML', 'Weather', 'Python']\n\n# Project to 2D using PCA\npca = PCA(n_components=2)\nall_embeddings_2d = pca.fit_transform(all_embeddings)\n\n# Create visualization\nplt.figure(figsize=(12, 8))\n\n# Plot original sentences (circles)\nfor i in range(len(sentences)):\n    x, y = all_embeddings_2d[i]\n    plt.scatter(x, y, c=topic_colors[all_topics[i]], s=150, alpha=0.7, \n               edgecolors='black', linewidth=2)\n    plt.annotate(f\"S{i+1}\", xy=(x, y), xytext=(5, 5), \n                textcoords='offset points', fontsize=10)\n\n# Plot new sentences (stars)\nfor i in range(len(sentences), len(sentences) + len(new_sentences)):\n    x, y = all_embeddings_2d[i]\n    plt.scatter(x, y, c=topic_colors[all_topics[i]], s=200, alpha=0.9, \n               marker='*', edgecolors='black', linewidth=2)\n    plt.annotate(f\"N{i-len(sentences)+1}\", xy=(x, y), xytext=(5, 5), \n                textcoords='offset points', fontsize=10, weight='bold')\n\n# Add a legend\nfor topic, color in topic_colors.items():\n    plt.scatter([], [], c=color, label=topic, s=100, alpha=0.7)\nplt.scatter([], [], c='gray', marker='o', s=100, label='Original', alpha=0.7, \n           edgecolors='black', linewidth=2)\nplt.scatter([], [], c='gray', marker='*', s=150, label='New', alpha=0.9, \n           edgecolors='black', linewidth=2)\nplt.legend(loc='lower right', fontsize=11)\n\nplt.title('PCA Projection: Original and New Sentences', fontsize=15, weight='bold')\nplt.xlabel(f'Principal Component 1 (Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=12)\nplt.ylabel(f'Principal Component 2 (Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=12)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Combined visualization complete!\")\nprint(\"\\nObservation:\")\nprint(\"  New sentences (stars) appear close to their semantically related\")\nprint(\"  original sentences, confirming the model captures meaning accurately.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've explored the fundamentals of text embeddings and their practical applications.\n\n### Key Takeaways\n\n1. **Embeddings represent meaning** - Text is converted to numerical vectors that capture semantic relationships\n2. **Similar meanings cluster together** - Semantically related sentences have high cosine similarity (>0.6)\n3. **Dimensionality matters** - Our model uses 384 dimensions to capture nuanced meaning\n4. **Visualization aids understanding** - PCA helps us see high-dimensional relationships in 2D\n5. **Transferability** - The model generalizes well to new sentences, finding appropriate matches\n\n### Real-World Applications\n\nEmbeddings power many modern AI applications:\n\n1. **Semantic Search** - Finding documents based on meaning rather than just keywords\n2. **Document Clustering** - Automatically grouping similar documents together\n3. **Recommendation Systems** - Suggesting similar items based on semantic content\n4. **Question Answering** - Finding relevant information to answer queries\n5. **Retrieval Augmented Generation (RAG)** - Combining LLMs with knowledge bases using embeddings\n\n### Next Steps\n\nThe techniques learned here form the foundation for:\n- Building semantic search systems\n- Creating document retrieval pipelines\n- Implementing RAG systems for enhanced LLM responses\n- Developing intelligent recommendation engines\n\nText embeddings bridge the gap between human language and machine understanding, enabling AI systems to work with meaning rather than just syntax."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}