{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Document Chunking Strategies\n\nChunking is the process of breaking down documents into smaller pieces that can be efficiently processed by language models and retrieval systems. In this notebook, we'll explore different chunking strategies and compare their impact on retrieval quality.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand why chunking is essential for RAG systems\n- Implement different chunking strategies (sentence, token, hierarchical, structure-aware)\n- Compare chunking approaches using retrieval experiments\n- Choose appropriate chunk sizes and overlap for your use case\n- Understand tradeoffs between chunk size, context, and precision\n\n## Why Chunking Matters\n\nThe way you chunk your documents directly impacts:\n\n- **Retrieval Precision** - How accurately your system finds relevant information\n- **Context Preservation** - How much surrounding information is maintained\n- **Token Economy** - How efficiently you use your LLM's context window\n- **Storage Requirements** - How much vector storage you need\n- **Search Quality** - Balance between granularity and coherence\n\nLet's explore different chunking strategies and their practical implications."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Sample Document Setup\n\nfrom llama_index.core.schema import Document\nimport textwrap\n\n# Sample document with clear structure\nsample_text = \"\"\"\n# Introduction to Vector Databases\n\nVector databases are specialized database systems designed to store and query vector embeddings efficiently.\nUnlike traditional databases optimized for exact matches, vector databases excel at similarity searches.\n\n## Key Advantages\n\nVector databases offer several advantages for AI applications:\n- Efficient similarity search using algorithms like HNSW and IVF\n- Support for high-dimensional vector data\n- Optimized for retrieval-augmented generation (RAG) applications\n\n## Common Operations\n\nThe most common operations in vector databases include:\n1. Adding vectors with associated metadata\n2. Searching for similar vectors using distance metrics\n3. Filtering results based on metadata\n4. Building and optimizing indexes for faster retrieval\n\n# Performance Considerations\n\nWhen working with vector databases at scale, consider:\n- Index construction time vs. query performance\n- Memory usage vs. search accuracy\n- Batch processing for efficient vector insertion\n\"\"\"\n\n# Create a Document\ndocument = Document(text=sample_text)\n\n# Display document info\nprint(\"Sample Document:\\n\")\nprint(\"=\" * 80)\nprint(f\"Total length: {len(document.text):,} characters\")\nprint(f\"Preview (first 200 chars):\\n\")\nprint(textwrap.fill(document.text[:200], 80))\nprint(\"...\\n\")\nprint(\"✓ Document loaded for chunking experiments\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Strategy 1: Sentence-Based Chunking\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Sentence-based chunking\nsentence_splitter = SentenceSplitter(\n    chunk_size=200,  # Target chunk size (in characters)\n    chunk_overlap=20  # Overlap between chunks (in characters)\n)\n\nsentence_nodes = sentence_splitter.get_nodes_from_documents([document])\n\nprint(\"Sentence-Based Chunking Results:\\n\")\nprint(\"=\" * 80)\nprint(f\"Chunks created: {len(sentence_nodes)}\\n\")\n\nprint(\"Sample chunks:\\n\")\nfor i in range(min(3, len(sentence_nodes))):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Length: {len(sentence_nodes[i].text)} characters\")\n    print(f\"  Text: {textwrap.fill(sentence_nodes[i].text[:150], 76)}...\")\n    print()\n\nprint(\"✓ Sentence-based chunking preserves natural sentence boundaries\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Strategy 2: Token-Based Chunking\n\nfrom llama_index.core.node_parser import TokenTextSplitter\n\n# Token-based chunking\ntoken_splitter = TokenTextSplitter(\n    chunk_size=100,  # Target chunk size (in tokens)\n    chunk_overlap=20  # Overlap between chunks (in tokens)\n)\n\ntoken_nodes = token_splitter.get_nodes_from_documents([document])\n\nprint(\"Token-Based Chunking Results:\\n\")\nprint(\"=\" * 80)\nprint(f\"Chunks created: {len(token_nodes)}\\n\")\n\nprint(\"Sample chunks:\\n\")\nfor i in range(min(3, len(token_nodes))):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Length: {len(token_nodes[i].text)} characters\")\n    print(f\"  Text: {textwrap.fill(token_nodes[i].text[:150], 76)}...\")\n    print()\n\nprint(\"✓ Token-based chunking ensures consistent token counts for LLM processing\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Strategy 3: Hierarchical Chunking\n\nfrom llama_index.core.node_parser import HierarchicalNodeParser\n\n# Hierarchical chunking with multiple levels\nhierarchical_splitter = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[512, 256, 128]  # Multi-level chunking from large to small\n)\n\nhierarchical_nodes = hierarchical_splitter.get_nodes_from_documents([document])\n\nprint(\"Hierarchical Chunking Results:\\n\")\nprint(\"=\" * 80)\nprint(f\"Chunks created: {len(hierarchical_nodes)}\\n\")\n\nprint(\"Sample chunks:\\n\")\nfor i in range(min(3, len(hierarchical_nodes))):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Length: {len(hierarchical_nodes[i].text)} characters\")\n    print(f\"  Text: {textwrap.fill(hierarchical_nodes[i].text[:150], 76)}...\")\n    print()\n\nprint(\"✓ Hierarchical chunking creates multi-level representations\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Strategy 4: Structure-Aware Chunking (Markdown)\n\nfrom llama_index.core.node_parser import MarkdownNodeParser\n\n# Structure-aware chunking for Markdown documents\nmarkdown_splitter = MarkdownNodeParser()\n\nmarkdown_nodes = markdown_splitter.get_nodes_from_documents([document])\n\nprint(\"Markdown-Aware Chunking Results:\\n\")\nprint(\"=\" * 80)\nprint(f\"Chunks created: {len(markdown_nodes)}\\n\")\n\nprint(\"Sample chunks with structure metadata:\\n\")\nfor i in range(min(3, len(markdown_nodes))):\n    header_path = markdown_nodes[i].metadata.get('header_path', 'N/A')\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Header path: {header_path}\")\n    print(f\"  Length: {len(markdown_nodes[i].text)} characters\")\n    print(f\"  Text: {textwrap.fill(markdown_nodes[i].text[:150], 76)}...\")\n    print()\n\nprint(\"✓ Markdown-aware chunking preserves document structure and hierarchy\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Install Embedding Model\n\nBefore we can compare retrieval quality, we need to install the embedding model library."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install embedding model library\n%env UV_LINK_MODE=copy\n!uv pip install llama-index-embeddings-huggingface"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Comparing Chunking Strategies with Retrieval\n\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n# Create a local embedding model\nprint(\"Loading embedding model...\")\nlocal_embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\nprint(\"✓ Embedding model loaded\\n\")\n\n# Create vector indexes with different chunking strategies\nprint(\"Creating vector indexes...\")\nsentence_index = VectorStoreIndex(sentence_nodes, embed_model=local_embed_model)\ntoken_index = VectorStoreIndex(token_nodes, embed_model=local_embed_model)\nmarkdown_index = VectorStoreIndex(markdown_nodes, embed_model=local_embed_model)\nprint(\"✓ Vector indexes created\\n\")\n\n# Test query\nquery = \"What are the common operations in vector databases?\"\n\n# Get retrieval results from each strategy\nprint(\"=\" * 80)\nprint(f\"Query: '{query}'\\n\")\nprint(\"=\" * 80)\n\nsentence_results = sentence_index.as_retriever(similarity_top_k=1).retrieve(query)\ntoken_results = token_index.as_retriever(similarity_top_k=1).retrieve(query)\nmarkdown_results = markdown_index.as_retriever(similarity_top_k=1).retrieve(query)\n\n# Compare results\nprint(\"\\n1. Sentence-based chunking result:\")\nprint(f\"   Score: {sentence_results[0].score:.4f}\")\nprint(f\"   Text: {textwrap.fill(sentence_results[0].node.text[:200], 76)}...\")\n\nprint(\"\\n2. Token-based chunking result:\")\nprint(f\"   Score: {token_results[0].score:.4f}\")\nprint(f\"   Text: {textwrap.fill(token_results[0].node.text[:200], 76)}...\")\n\nprint(\"\\n3. Markdown-aware chunking result:\")\nprint(f\"   Score: {markdown_results[0].score:.4f}\")\nprint(f\"   Text: {textwrap.fill(markdown_results[0].node.text[:200], 76)}...\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Retrieval comparison complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've explored four different chunking strategies and their impact on retrieval:\n\n### Chunking Strategies Compared\n\n1. **Sentence-Based Chunking**\n   - Preserves natural sentence boundaries\n   - Good for maintaining semantic coherence\n   - Variable chunk sizes\n   - Best for: General text documents with clear sentence structure\n\n2. **Token-Based Chunking**\n   - Consistent token counts for LLM processing\n   - Predictable memory usage\n   - May split mid-sentence\n   - Best for: Token-limited scenarios, cost optimization\n\n3. **Hierarchical Chunking**\n   - Multi-level representations (coarse to fine)\n   - Enables multi-scale retrieval\n   - More complex to implement\n   - Best for: Large documents, when you need both overview and detail\n\n4. **Structure-Aware Chunking**\n   - Respects document structure (headings, sections)\n   - Preserves hierarchical metadata\n   - Format-specific (Markdown, HTML)\n   - Best for: Structured documents with clear organization\n\n### Key Tradeoffs\n\n**Chunk Size:**\n- **Smaller chunks** → More precise retrieval, less context, more storage\n- **Larger chunks** → More context, less precision, fewer chunks\n\n**Chunk Overlap:**\n- **More overlap** → Better boundary handling, more storage, potential duplication\n- **Less overlap** → Efficient storage, risk of missing boundary information\n\n**Structure Awareness:**\n- **Structure-aware** → Better semantic coherence, format-specific\n- **Structure-agnostic** → Simpler, works with any text, may break logical units\n\n### Best Practices\n\n1. **Start with structure-aware chunking** if your documents have clear structure\n2. **Use 200-500 tokens** as a starting chunk size for most applications\n3. **Add 10-20% overlap** to handle boundary cases\n4. **Test with actual queries** to validate your chunking strategy\n5. **Consider your use case**:\n   - Q&A systems: Smaller chunks (100-300 tokens)\n   - Summarization: Larger chunks (500-1000 tokens)\n   - General RAG: Medium chunks (200-500 tokens)\n\n6. **Monitor retrieval quality** and adjust based on results\n\nChunking strategy significantly impacts RAG system performance - invest time in testing different approaches with your specific documents and queries."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}