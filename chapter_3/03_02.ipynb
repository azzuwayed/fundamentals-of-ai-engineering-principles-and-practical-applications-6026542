{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Text Extraction Fundamentals\n\nIn this notebook, we'll explore how to extract text from various document formats using LlamaIndex readers.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Extract text from different file formats (PDF, DOCX, CSV, JSON, Markdown, databases)\n- Clean and normalize extracted text\n- Extract metadata from documents\n- Create a universal document processing pipeline\n\n## Why Text Extraction Matters\n\nBefore we can build AI applications that work with documents, we need to extract and process the text. Different file formats require different extraction approaches, and the quality of extraction directly impacts downstream AI tasks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Extracting from Common Document Formats\n\nfrom llama_index.readers.file import PDFReader, DocxReader\nfrom llama_index.readers.web import SimpleWebPageReader\nimport pathlib\n\n# Extract from PDF\npdf_reader = PDFReader()\npdf_docs = pdf_reader.load_data(file=pathlib.Path(\"samples/pdf-report.pdf\"))\n\n# Extract from DOCX\ndocx_reader = DocxReader()\ndocx_docs = docx_reader.load_data(file=pathlib.Path(\"samples/docx-report.docx\"))\n\n# Extract from Web\nweb_reader = SimpleWebPageReader()\nweb_docs = web_reader.load_data(urls=[\"https://example.com\"])\n\n# Display extracted text samples\nprint(\"Extracted Text Samples:\\n\")\nprint(\"=\" * 80)\nprint(f\"\\nPDF extract ({len(pdf_docs[0].text)} chars):\")\nprint(f\"  {pdf_docs[0].text[:150]}...\\n\")\n\nprint(f\"DOCX extract ({len(docx_docs[0].text)} chars):\")\nprint(f\"  {docx_docs[0].text[:150]}...\\n\")\n\nprint(f\"Web extract ({len(web_docs[0].text)} chars):\")\nprint(f\"  {web_docs[0].text[:150]}...\")\n\nprint(\"\\n✓ Successfully extracted text from PDF, DOCX, and web sources\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Extracting from Data Formats\n\nfrom llama_index.readers.file import CSVReader, MarkdownReader\nfrom llama_index.readers.json import JSONReader\nfrom llama_index.readers.database import DatabaseReader\nimport pathlib\n\n# CSV files\ncsv_reader = CSVReader()\ncsv_docs = csv_reader.load_data(file=pathlib.Path(\"samples/csv-data.csv\"))\n\n# JSON files\njson_reader = JSONReader()\njson_docs = json_reader.load_data(input_file=\"samples/json-data.json\")\n\n# Markdown files\nmd_reader = MarkdownReader()\nmd_docs = md_reader.load_data(file=\"samples/README.md\")\n\n# Databases\ndb_reader = DatabaseReader(uri=\"sqlite:///samples/database.db\")\ndb_docs = db_reader.load_data(query=\"SELECT * FROM orders\")\n\n# Display extracted text samples\nprint(\"Structured Data Extraction:\\n\")\nprint(\"=\" * 80)\n\nprint(f\"\\nCSV extract ({len(csv_docs[0].text)} chars):\")\nprint(f\"  {csv_docs[0].text[:150]}...\\n\")\n\nprint(f\"JSON extract ({len(json_docs[0].text)} chars):\")\nprint(f\"  {json_docs[0].text[:150]}...\\n\")\n\nprint(f\"Markdown extract ({len(md_docs[0].text)} chars):\")\nprint(f\"  {md_docs[0].text[:150]}...\\n\")\n\nprint(f\"Database extract ({len(db_docs[0].text)} chars):\")\nprint(f\"  {db_docs[0].text[:150]}...\")\n\nprint(\"\\n✓ Successfully extracted text from CSV, JSON, Markdown, and database\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Text Cleaning and Normalization\n\nimport re\nfrom llama_index.core.schema import Document\n\n# Get raw text from a document\nraw_text = pdf_docs[0].text\n\ndef clean_text(text):\n    \"\"\"\n    Clean and normalize extracted text.\n    \n    Args:\n        text (str): Raw text to clean\n        \n    Returns:\n        str: Cleaned text\n    \"\"\"\n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Remove special characters but keep structural elements\n    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\-\\(\\)\\[\\]\\{\\}\\\"\\'\\n\\t]', '', text)\n\n    # Fix common OCR errors (example)\n    text = text.replace('l<eywor', 'keyword')\n\n    return text.strip()\n\n# Clean the text\ncleaned_text = clean_text(raw_text)\n\n# Compare original vs cleaned\nprint(\"Text Cleaning Comparison:\\n\")\nprint(\"=\" * 80)\nprint(f\"\\nOriginal (first 100 chars):\")\nprint(f\"  {raw_text[:100]}\\n\")\n\nprint(f\"Cleaned (first 100 chars):\")\nprint(f\"  {cleaned_text[:100]}\\n\")\n\nprint(f\"Length comparison:\")\nprint(f\"  Original: {len(raw_text):,} characters\")\nprint(f\"  Cleaned:  {len(cleaned_text):,} characters\")\nprint(f\"  Removed:  {len(raw_text) - len(cleaned_text)} characters\")\n\nprint(\"\\n✓ Text cleaned and normalized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Metadata Extraction\n\ndef extract_metadata(text, filename):\n    \"\"\"\n    Extract metadata from document text.\n    \n    Args:\n        text (str): Document text\n        filename (str): Source filename\n        \n    Returns:\n        dict: Extracted metadata\n    \"\"\"\n    metadata = {\n        \"source\": filename,\n        \"file_type\": filename.split('.')[-1],\n    }\n\n    # Extract title (assume first line might be title)\n    lines = text.split('\\n')\n    if lines and len(lines[0]) < 100:  # Simple heuristic for title\n        metadata[\"title\"] = lines[0].strip()\n\n    # Try to extract date with regex\n    date_match = re.search(r'\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}', text)\n    if date_match:\n        metadata[\"date\"] = date_match.group(0)\n\n    return metadata\n\n# Extract metadata\nmetadata = extract_metadata(raw_text, \"samples/pdf-report.pdf\")\n\n# Create a document with cleaned text and metadata\nprocessed_doc = Document(\n    text=cleaned_text,\n    metadata=metadata\n)\n\nprint(\"Extracted Metadata:\\n\")\nprint(\"=\" * 80)\nfor key, value in metadata.items():\n    print(f\"  {key:<12}: {value}\")\n\nprint(\"\\n✓ Metadata extracted and attached to document\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Universal Document Processing Pipeline\n\ndef process_document(file_path):\n    \"\"\"\n    Process a document with appropriate reader and cleaning.\n    \n    Args:\n        file_path (str): Path to the document\n        \n    Returns:\n        Document: Processed LlamaIndex Document object\n    \"\"\"\n    # Determine file type\n    file_type = file_path.split('.')[-1].lower()\n\n    # Select appropriate reader\n    if file_type == 'pdf':\n        reader = PDFReader()\n    elif file_type in ['docx', 'doc']:\n        reader = DocxReader()\n    elif file_type in ['html', 'htm']:\n        reader = SimpleWebPageReader()\n    else:\n        # Default to simple text reading\n        with open(file_path, 'r') as f:\n            return Document(text=f.read(), metadata={\"source\": file_path})\n\n    # Load and extract text\n    docs = reader.load_data(file=file_path)\n\n    if not docs:\n        return None\n\n    # Clean the text\n    cleaned_text = clean_text(docs[0].text)\n\n    # Extract metadata\n    metadata = extract_metadata(docs[0].text, file_path)\n\n    # Create processed document\n    return Document(text=cleaned_text, metadata=metadata)\n\n# Test the pipeline\nprocessed_doc = process_document(\"samples/pdf-report.pdf\")\n\nprint(\"Document Processing Pipeline:\\n\")\nprint(\"=\" * 80)\nprint(f\"\\nProcessed document:\")\nprint(f\"  Characters: {len(processed_doc.text):,}\")\nprint(f\"  Metadata:   {processed_doc.metadata}\")\n\nprint(\"\\n✓ Universal processing pipeline ready for use\")\nprint(\"\\nThis pipeline can be used with any supported document format,\")\nprint(\"automatically selecting the right reader and applying consistent\")\nprint(\"cleaning and metadata extraction.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nWe've covered the complete text extraction pipeline:\n\n### Key Components\n\n1. **Format-Specific Readers** - LlamaIndex provides readers for common formats:\n   - Documents: PDF, DOCX\n   - Web: HTML pages\n   - Data: CSV, JSON, Markdown\n   - Databases: SQL queries\n\n2. **Text Cleaning** - Normalize extracted text by:\n   - Removing excessive whitespace\n   - Filtering special characters\n   - Fixing common OCR errors\n\n3. **Metadata Extraction** - Enrich documents with:\n   - Source information\n   - File type\n   - Titles and dates\n   - Custom metadata\n\n4. **Universal Pipeline** - A single function that:\n   - Auto-detects file type\n   - Selects appropriate reader\n   - Applies consistent cleaning\n   - Extracts metadata\n\n### Best Practices\n\n- **Choose the right reader** for each format\n- **Clean consistently** to avoid downstream issues\n- **Extract metadata** to enrich your documents\n- **Build pipelines** for reusable, maintainable code\n\nThis foundation enables building robust AI applications that work with real-world documents.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}