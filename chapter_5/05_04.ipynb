{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ChromaDB Scaling Strategies with ANN\n\nThis notebook explores scaling strategies using Approximate Nearest Neighbor (ANN) algorithms for production-scale vector databases.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Understand ANN algorithms and their trade-offs\n- Configure HNSW parameters for different use cases\n- Benchmark query performance across different configurations\n- Choose optimal settings for speed vs. accuracy requirements\n- Scale vector databases to handle thousands of documents efficiently\n\n## Key Scaling Considerations\n\n1. **Speed vs. Accuracy** - Trade-offs between query performance and result quality\n2. **Resource Limitations** - Managing memory, CPU, and storage constraints\n3. **Index Configuration** - Tuning HNSW parameters for optimal performance\n4. **Production Requirements** - Meeting real-world SLA requirements"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What is HNSW?\n\n**HNSW (Hierarchical Navigable Small World)** is an ANN algorithm that enables fast similarity search:\n\n- **Approximate** - Trades perfect accuracy for speed (typically 95-99% accuracy)\n- **Hierarchical** - Multi-layer graph structure for efficient navigation\n- **Navigable** - Optimized path-finding through the vector space\n- **Small World** - Few hops needed to reach any node\n\n**Why use ANN?**\n- Exact nearest neighbor search is O(n) - too slow for large datasets\n- ANN algorithms achieve O(log n) or better with minimal accuracy loss\n- Essential for production systems with millions of vectors"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Install Required Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['UV_LINK_MODE'] = 'copy'\n\n!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2\n\nprint(\"✓ Required libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import chromadb\nfrom chromadb.utils import embedding_functions\nimport time\n\n# Initialize ChromaDB\nclient = chromadb.Client()\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=\"all-MiniLM-L6-v2\"\n)\n\nprint(\"✓ ChromaDB initialized!\")\nprint(f\"  Embedding model: all-MiniLM-L6-v2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## HNSW Parameter Configurations\n\nWe'll create three collections with different HNSW settings to compare performance:\n\n### HNSW Parameters Explained\n\n| Parameter | Description | Effect |\n|-----------|-------------|--------|\n| `hnsw:space` | Distance metric (cosine, euclidean, etc.) | How similarity is calculated |\n| `hnsw:construction_ef` | Build quality (higher = better) | Index construction time |\n| `hnsw:search_ef` | Search quality (higher = more accurate) | Query latency |\n| `hnsw:M` | Max connections per node | Memory usage & accuracy |\n\n### Our Configurations\n\n1. **Default** - ChromaDB defaults (balanced)\n2. **High Accuracy** - Prioritizes quality (construction_ef=1000, search_ef=1250, M=36)\n3. **Fast Search** - Prioritizes speed (construction_ef=80, search_ef=40, M=12)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create collections with different HNSW configurations\ncollections = {}\n\nprint(\"Creating collections with different HNSW configurations...\\n\")\n\n# 1. Default settings\ncollections[\"default\"] = client.create_collection(\n    name=\"default_index\",\n    embedding_function=embedding_function\n)\nprint(\"✓ Created 'default' collection (ChromaDB defaults)\")\n\n# 2. High accuracy configuration\ncollections[\"high_accuracy\"] = client.create_collection(\n    name=\"high_accuracy_index\",\n    embedding_function=embedding_function,\n    metadata={\n        \"hnsw:space\": \"cosine\",\n        \"hnsw:construction_ef\": 1000,\n        \"hnsw:search_ef\": 1250,\n        \"hnsw:M\": 36\n    }\n)\nprint(\"✓ Created 'high_accuracy' collection (quality-optimized)\")\n\n# 3. Fast search configuration\ncollections[\"fast_search\"] = client.create_collection(\n    name=\"fast_search_index\",\n    embedding_function=embedding_function,\n    metadata={\n        \"hnsw:space\": \"cosine\",\n        \"hnsw:construction_ef\": 80,\n        \"hnsw:search_ef\": 40,\n        \"hnsw:M\": 12\n    }\n)\nprint(\"✓ Created 'fast_search' collection (speed-optimized)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Documents\n",
    "\n",
    "Now let's create some sample documents across different categories to populate our collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate sample documents\nnum_docs = 10000\ncategories = [\"technology\", \"science\", \"health\", \"business\", \"entertainment\"]\n\nprint(f\"Generating {num_docs:,} sample documents...\")\n\ndocuments = []\nids = []\n\nfor i in range(num_docs):\n    category = categories[i % len(categories)]\n    document = f\"This is document {i} about {category} with additional text for uniqueness.\"\n    documents.append(document)\n    ids.append(f\"doc_{i}\")\n\nprint(f\"✓ Generated {num_docs:,} documents across {len(categories)} categories\")\nprint(f\"\\nSample documents:\")\nfor i in range(3):\n    print(f\"  {i+1}. {documents[i]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Documents to Collections\n",
    "\n",
    "Let's add the generated documents to all three collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"INDEXING BENCHMARK\")\nprint(\"=\" * 80)\n\nprint(f\"\\nAdding {num_docs:,} documents to each collection...\\n\")\n\nfor name, collection in collections.items():\n    start_time = time.time()\n    collection.add(documents=documents, ids=ids)\n    elapsed_time = time.time() - start_time\n    \n    print(f\"✓ {name:15} → {elapsed_time:6.2f}s ({num_docs/elapsed_time:6.0f} docs/sec)\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Query Performance\n",
    "\n",
    "Now let's evaluate how each configuration performs with a set of representative queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"QUERY PERFORMANCE BENCHMARK\")\nprint(\"=\" * 80)\n\n# Test queries\nquery_texts = [\n    \"Latest technology trends in artificial intelligence\",\n    \"Scientific research on climate change\",\n    \"Health benefits of regular exercise\",\n    \"Business strategies for startups\",\n    \"Entertainment news about recent movie releases\"\n]\n\nnum_trials = 5\nresults = {}\n\nprint(f\"\\nRunning {num_trials} trials per query across {len(query_texts)} queries...\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark each configuration\nfor name, collection in collections.items():\n    print(f\"Testing {name}:\")\n    times = []\n    \n    for query in query_texts:\n        query_times = []\n        \n        for _ in range(num_trials):\n            start_time = time.time()\n            collection.query(query_texts=[query], n_results=10)\n            query_times.append(time.time() - start_time)\n        \n        avg_time = sum(query_times) / len(query_times)\n        times.append(avg_time)\n        print(f\"  '{query[:35]}...' → {avg_time*1000:5.1f}ms\")\n    \n    results[name] = {\n        \"mean\": sum(times) / len(times),\n        \"min\": min(times),\n        \"max\": max(times),\n        \"times\": times\n    }\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"PERFORMANCE SUMMARY\")\nprint(\"=\" * 80)\n\nfor name, metrics in results.items():\n    mean_ms = metrics['mean'] * 1000\n    min_ms = metrics['min'] * 1000\n    max_ms = metrics['max'] * 1000\n    print(f\"\\n{name:15}\")\n    print(f\"  Mean: {mean_ms:5.1f}ms\")\n    print(f\"  Min:  {min_ms:5.1f}ms\")\n    print(f\"  Max:  {max_ms:5.1f}ms\")\n\n# Calculate relative performance\nbaseline = results['default']['mean']\nprint(f\"\\nRelative to default:\")\nfor name in ['high_accuracy', 'fast_search']:\n    ratio = results[name]['mean'] / baseline\n    direction = \"slower\" if ratio > 1 else \"faster\"\n    print(f\"  {name:15} → {abs(1-ratio)*100:4.1f}% {direction}\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've explored ANN scaling strategies using HNSW configurations in ChromaDB.\n\n### Key Takeaways\n\n1. **HNSW enables scale** - Handles 10,000+ documents with sub-15ms queries\n2. **Configuration matters** - Parameters significantly impact speed vs. accuracy\n3. **Trade-offs are real**:\n   - **High accuracy** → Slower queries, better results\n   - **Fast search** → Faster queries, slightly less accurate\n   - **Default** → Balanced for most use cases\n\n4. **Parameter guide**:\n   - `construction_ef`: Higher = better index quality (longer build time)\n   - `search_ef`: Higher = more accurate search (slower queries)\n   - `M`: Higher = more connections (more memory, better accuracy)\n\n### Configuration Recommendations\n\n**Choose High Accuracy when:**\n- Result quality is critical\n- Latency <50ms is acceptable\n- Use cases: Medical search, legal research, critical recommendations\n\n**Choose Fast Search when:**\n- Speed is paramount\n- Slight accuracy loss is acceptable\n- Use cases: Real-time search, autocomplete, high-QPS services\n\n**Choose Default when:**\n- Balanced performance needed\n- Standard production requirements\n- Use cases: Most semantic search applications\n\n### Production Best Practices\n\n1. **Benchmark with real data** - Synthetic data doesn't reflect production patterns\n2. **Monitor accuracy** - Track recall@k metrics to ensure quality\n3. **Start conservative** - Begin with default or high accuracy, optimize later\n4. **Load test** - Verify performance under concurrent load\n5. **Document choices** - Record configuration decisions and reasoning\n\n### Next Steps\n\n- Test with your domain-specific queries\n- Experiment with different parameter combinations\n- Implement recall metrics to measure accuracy\n- Consider horizontal scaling for very large datasets (millions of vectors)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}