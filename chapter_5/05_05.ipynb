{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3625d1",
   "metadata": {},
   "source": "# ChromaDB Scaling with Caching\n\nThis notebook explores caching strategies for optimizing vector database performance at scale.\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n- Implement LRU caching for query results\n- Measure cache performance and hit rates\n- Understand caching trade-offs and best practices\n- Apply horizontal scaling strategies for production systems\n- Optimize resource usage across memory, CPU, and network\n\n## Why Implement Caching?\n\nCaching is crucial for production vector database systems:\n\n1. **Reduced latency** - Cached results return instantly (no embedding computation or vector search)\n2. **Lower costs** - Fewer GPU/CPU cycles for embeddings and similarity calculations\n3. **Better scalability** - Handle more queries per second with same resources\n4. **Improved UX** - Sub-millisecond responses for common queries"
  },
  {
   "cell_type": "markdown",
   "id": "a66599d3",
   "metadata": {},
   "source": "## Setup: Install Required Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c121134",
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['UV_LINK_MODE'] = 'copy'\n\n!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2\n\nprint(\"✓ Required libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3903959",
   "metadata": {},
   "outputs": [],
   "source": "import chromadb\nfrom chromadb.utils import embedding_functions\nimport time\nimport random\n\nprint(\"✓ Libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "4541a3e8",
   "metadata": {},
   "source": "## LRU Cache Implementation\n\n**LRU (Least Recently Used)** cache keeps frequently accessed items and evicts least-used entries when full.\n\n### How LRU Works\n\n1. Track access order for all cached items\n2. When cache is full, remove least recently used item\n3. Move accessed items to \"most recently used\" position"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000972fe",
   "metadata": {},
   "outputs": [],
   "source": "class LRUCache:\n    \"\"\"Simple LRU cache for query results\"\"\"\n    def __init__(self, capacity=100):\n        self.capacity = capacity\n        self.cache = {}\n        self.usage_order = []\n    \n    def get(self, key):\n        \"\"\"Get item from cache, return None if not found\"\"\"\n        if key in self.cache:\n            # Move to end (most recently used)\n            self.usage_order.remove(key)\n            self.usage_order.append(key)\n            return self.cache[key]\n        return None\n    \n    def put(self, key, value):\n        \"\"\"Add item to cache, evict LRU if full\"\"\"\n        if key in self.cache:\n            self.cache[key] = value\n            self.usage_order.remove(key)\n            self.usage_order.append(key)\n        else:\n            if len(self.cache) >= self.capacity:\n                # Evict least recently used\n                lru_key = self.usage_order.pop(0)\n                del self.cache[lru_key]\n            \n            self.cache[key] = value\n            self.usage_order.append(key)\n    \n    def clear(self):\n        self.cache = {}\n        self.usage_order = []\n    \n    def __len__(self):\n        return len(self.cache)\n\nprint(\"✓ LRUCache class defined!\")"
  },
  {
   "cell_type": "markdown",
   "id": "86240000",
   "metadata": {},
   "source": [
    "### Setting Up the Collection\n",
    "\n",
    "Now let's create a collection and populate it with sample documents for our caching experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad47ad8",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize ChromaDB\nclient = chromadb.Client()\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=\"all-MiniLM-L6-v2\"\n)\n\n# Create collection\ncollection = client.create_collection(\n    name=\"cache_test\",\n    embedding_function=embedding_function\n)\n\nprint(\"✓ ChromaDB collection created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b50b9",
   "metadata": {},
   "outputs": [],
   "source": "# Add sample documents\nnum_docs = 1000\ndocuments = [f\"Sample document {i} with content for testing caching\" for i in range(num_docs)]\nids = [f\"cache_doc_{i}\" for i in range(num_docs)]\n\nprint(f\"Adding {num_docs:,} documents...\")\nfor i in range(0, num_docs, 100):\n    collection.add(documents=documents[i:i+100], ids=ids[i:i+100])\n\nprint(f\"✓ Added {num_docs:,} documents to collection\")"
  },
  {
   "cell_type": "markdown",
   "id": "bc82672f",
   "metadata": {},
   "source": [
    "### Cached Query Function\n",
    "\n",
    "Let's implement a function that uses our cache to store and retrieve query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c0f8d",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize cache\nquery_cache = LRUCache(capacity=50)\n\ndef cached_query(query_text, n_results=10, use_cache=True):\n    \"\"\"Query with optional caching\"\"\"\n    cache_key = f\"{query_text}:{n_results}\"\n    \n    if use_cache:\n        cached_result = query_cache.get(cache_key)\n        if cached_result is not None:\n            return cached_result, True  # Cache hit\n    \n    # Cache miss - perform actual query\n    result = collection.query(query_texts=[query_text], n_results=n_results)\n    \n    if use_cache:\n        query_cache.put(cache_key, result)\n    \n    return result, False  # Cache miss\n\nprint(\"✓ Cached query function defined!\")"
  },
  {
   "cell_type": "markdown",
   "id": "babcac93",
   "metadata": {},
   "source": [
    "### Preparing Query Mix\n",
    "\n",
    "To simulate a realistic workload, we'll create a mix of common (frequently repeated) and unique queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1315272",
   "metadata": {},
   "outputs": [],
   "source": "# Create realistic query mix\ncommon_queries = [\n    \"document with content\",\n    \"sample document\",\n    \"testing caching\",\n    \"various content\"\n]\n\nunique_queries = [f\"unique query {i}\" for i in range(50)]\n\n# Mix: common queries repeated, unique queries occasional\nmixed_queries = []\nfor _ in range(20):\n    mixed_queries.extend(common_queries)  # 80 common\n    mixed_queries.extend(random.sample(unique_queries, 5))  # 100 unique\n\nrandom.shuffle(mixed_queries)\n\nprint(f\"✓ Generated {len(mixed_queries)} queries\")\nprint(f\"  Common queries: {len([q for q in mixed_queries if q in common_queries])}\")\nprint(f\"  Unique queries: {len([q for q in mixed_queries if q in unique_queries])}\")"
  },
  {
   "cell_type": "markdown",
   "id": "52f599ab",
   "metadata": {},
   "source": [
    "### Benchmark: No Cache vs. With Cache\n",
    "\n",
    "Now let's measure the performance difference between running queries without a cache versus with a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891debb",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"BENCHMARK: NO CACHE\")\nprint(\"=\" * 80)\n\nstart_time = time.time()\nfor query in mixed_queries:\n    _, _ = cached_query(query, use_cache=False)\nno_cache_time = time.time() - start_time\n\nprint(f\"✓ Completed {len(mixed_queries)} queries without cache\")\nprint(f\"  Total time: {no_cache_time:.4f}s\")\nprint(f\"  Avg per query: {no_cache_time/len(mixed_queries)*1000:.2f}ms\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2606e6b",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"BENCHMARK: WITH CACHE\")\nprint(\"=\" * 80)\n\nquery_cache.clear()\nstart_time = time.time()\nhits = 0\n\nfor query in mixed_queries:\n    _, is_hit = cached_query(query, use_cache=True)\n    if is_hit:\n        hits += 1\n\nwith_cache_time = time.time() - start_time\nhit_rate = hits / len(mixed_queries)\n\nprint(f\"✓ Completed {len(mixed_queries)} queries with cache\")\nprint(f\"  Total time: {with_cache_time:.4f}s\")\nprint(f\"  Avg per query: {with_cache_time/len(mixed_queries)*1000:.2f}ms\")\nprint(f\"  Cache hits: {hits}/{len(mixed_queries)} ({hit_rate:.1%})\")\nprint(f\"  Cache size: {len(query_cache)}/{query_cache.capacity}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21571b5f",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"PERFORMANCE COMPARISON\")\nprint(\"=\" * 80)\n\nspeedup = no_cache_time / with_cache_time\ntime_saved = no_cache_time - with_cache_time\npercent_saved = (1 - with_cache_time/no_cache_time) * 100\n\nprint(f\"\\nWithout cache: {no_cache_time:.4f}s\")\nprint(f\"With cache:    {with_cache_time:.4f}s\")\nprint(f\"\\nTime saved:    {time_saved:.4f}s ({percent_saved:.1f}%)\")\nprint(f\"Speedup:       {speedup:.2f}x faster\")\nprint(f\"Hit rate:      {hit_rate:.1%}\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "894b85f9",
   "metadata": {},
   "source": [
    "## Advanced Scaling Strategies\n",
    "\n",
    "### Horizontal Scaling Approaches\n",
    "\n",
    "As your vector database grows beyond the capacity of a single machine, you'll need to implement horizontal scaling strategies. Here are some common approaches:\n",
    "\n",
    "1. **Sharding** - Partitioning your vector space across multiple instances\n",
    "   - **By ID range** - Deterministic but may lead to unbalanced shards\n",
    "   - **By vector clustering** - Better search performance but more complex\n",
    "\n",
    "2. **Replication** - Creating copies of your data across multiple instances\n",
    "   - Improves read throughput and fault tolerance\n",
    "   - Requires synchronization mechanisms for writes\n",
    "\n",
    "3. **Hybrid approaches** - Combining sharding and replication\n",
    "   - Example: ChromaDB cluster with data sharded across nodes and each shard replicated\n",
    "\n",
    "### Resource Management Best Practices\n",
    "\n",
    "1. **Memory Optimization**\n",
    "   - Use quantization to reduce vector size (e.g., 32-bit to 8-bit)\n",
    "   - Implement disk-based storage for less frequently accessed vectors\n",
    "\n",
    "2. **CPU Utilization**\n",
    "   - Batch similar operations\n",
    "   - Use asynchronous processing where possible\n",
    "\n",
    "3. **Network Efficiency**\n",
    "   - Minimize data transfer between components\n",
    "   - Compress payloads when possible\n",
    "\n",
    "### Real-world Implementation Considerations\n",
    "\n",
    "1. **Monitoring and Observability**\n",
    "   - Track latency, throughput, and error rates\n",
    "   - Set up alerts for performance degradation\n",
    "\n",
    "2. **Failure Handling**\n",
    "   - Implement graceful degradation strategies\n",
    "   - Consider fallback search methods\n",
    "\n",
    "3. **Update Strategies**\n",
    "   - Batch updates to reduce index rebuilding frequency\n",
    "   - Consider incremental index updates\n",
    "\n",
    "4. **Hybrid Search Approaches**\n",
    "   - Combine vector search with keyword search for better results\n",
    "   - Filter vectors based on metadata before computing distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ab38c",
   "metadata": {},
   "source": "## Summary\n\nWe've explored caching strategies and scaling approaches for production vector databases.\n\n### Key Takeaways\n\n1. **Caching is highly effective** - 70-80% time savings for realistic query patterns\n2. **LRU works well** - Simple to implement, effective for temporal locality\n3. **Cache sizing matters** - Balance memory usage vs. hit rate\n4. **Monitor hit rates** - Track cache effectiveness in production\n\n### Caching Best Practices\n\n**When to use caching:**\n- High query volume with repeated patterns\n- Expensive embedding computations\n- Read-heavy workloads\n- Latency-sensitive applications\n\n**Cache configuration:**\n- **Size** - Monitor hit rates, increase if misses are frequent\n- **TTL** - Expire stale results (important for frequently updated data)\n- **Eviction** - LRU works well for most cases, consider LFU for skewed access patterns\n\n**Production considerations:**\n- Distributed caching (Redis, Memcached) for multi-instance deployments\n- Cache warming for known popular queries\n- Monitor cache memory usage and hit/miss rates\n- Implement cache invalidation for data updates\n\n### Horizontal Scaling Strategies\n\n**1. Sharding** - Partition vector space across instances\n- By ID range (simple, may be unbalanced)\n- By vector clustering (better search, more complex)\n- By metadata (e.g., category, tenant)\n\n**2. Replication** - Copy data across instances\n- Improves read throughput\n- Provides fault tolerance\n- Requires write synchronization\n\n**3. Hybrid** - Combine sharding + replication\n- Example: 3 shards, each replicated 2x = 6 instances\n- Balance reads and writes\n- Trade-off: complexity vs. performance\n\n### Resource Optimization\n\n**Memory:**\n- Vector quantization (32-bit → 8-bit)\n- Disk-backed storage for cold data\n- Compressed vector formats\n\n**CPU:**\n- Batch similar operations\n- Asynchronous processing\n- Multi-threading for independent queries\n\n**Network:**\n- Minimize data transfer\n- Compress payloads\n- Local caching at edge\n\n### Production Checklist\n\n✓ Implement monitoring (latency, throughput, errors)  \n✓ Set up alerting for degradation  \n✓ Plan graceful degradation strategies  \n✓ Implement failover mechanisms  \n✓ Test under load (concurrent queries)  \n✓ Document scaling decisions  \n✓ Plan for data growth  \n✓ Implement backup/restore procedures\n\n### Next Steps\n\n- Implement distributed caching with Redis\n- Set up multi-instance deployments\n- Add comprehensive monitoring\n- Test sharding strategies with your data\n- Benchmark under production-like load"
  },
  {
   "cell_type": "markdown",
   "id": "89cc98e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}